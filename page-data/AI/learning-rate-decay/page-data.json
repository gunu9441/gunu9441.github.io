{"componentChunkName":"component---src-templates-blog-post-js","path":"/AI/learning-rate-decay/","result":{"data":{"site":{"siteMetadata":{"title":"Gunu's AI Log","author":"[Gunu]","siteUrl":"https://gunu441.github.io","comment":{"disqusShortName":"","utterances":"gunu9441/gunu9441.github.io"},"sponsor":{"buyMeACoffeeId":"gunu9441"}}},"markdownRemark":{"id":"0543a55d-c18a-5249-a500-18e26af4c512","excerpt":"이번 시간에는 learning rate decay에 대해서 알아보도록 하겠습니다!😆 Learning rate는 hyperparameter중 가장 중요하다고 말할 수 있습니다. 이 learning rate 에 따라서 학습속도가 결정되며 Global minimum으로 수렴할 수 있느냐를 결정하는 중요한 요소입니다. 그래서 오늘은 이 중요한 learning rate와 관련된 주제인 learning rate decay에 대해서 알아보도록 하겠습니다. 다음은 이번에 배울 learning rate…","html":"<p>    이번 시간에는 <strong>learning rate decay</strong>에 대해서 알아보도록 하겠습니다!😆 Learning rate는 <strong>hyperparameter중 가장 중요</strong>하다고 말할 수 있습니다. 이 learning rate <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>에 따라서 <strong>학습속도가 결정</strong>되며 <strong>Global minimum으로 수렴할 수 있느냐를 결정하는 중요한 요소</strong>입니다. 그래서 오늘은 이 중요한 learning rate와 관련된 주제인 <strong>learning rate decay</strong>에 대해서 알아보도록 하겠습니다. 다음은 이번에 배울 learning rate에 대한 Category입니다!</p>\n<h2 id=\"category\" style=\"position:relative;\"><a href=\"#category\" aria-label=\"category permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Category</h2>\n<ol>\n<li>Learning Rate</li>\n<li>Learning Rate Decay</li>\n<li>The Kinds of Learning Rate Decay</li>\n<li>Summary</li>\n<li>Reference</li>\n</ol>\n<h2 id=\"learning-rate\" style=\"position:relative;\"><a href=\"#learning-rate\" aria-label=\"learning rate permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Learning Rate</h2>\n<p>    이전 포스트 ’<strong>Fancier Optimization</strong>‘을 보게되면 다양한 Optimization algorithm들을 볼 수 있습니다. 기존에 사용했던 vanilla SGD는 각 차원(<strong>각 paramter를 하나의 차원</strong>으로 봅니다!)의 <strong>gradient를 고려하지 않기 때문</strong>에 <strong>Loss 값이 smooth하게 minimum으로 수렴</strong>하는 것이 아니라 <strong>zigzag로 수렴</strong>하게 됩니다. 즉, loss가 <strong>각 파라미터의 gradient값에 대한 민감도가 다르기 때문</strong>에 <strong>각 차원별로 이동하는 방향에 대한 크기에 달라</strong>지게되어 <strong>zigzag를 그리면서 수렴</strong>하게 됩니다. 또한, 현재의 gradient 값만을 생각해서 작동하기 때문에 <strong>saddle point</strong>나 <strong>local minimum</strong>에 걸리게 됩니다. 이에 만들어진 fancier optimization에는 vanilla SGD를 기반으로 만들어진 <strong>SGD+Momentum</strong>, <strong>AdaGrad</strong>, <strong>RMSProp</strong>, <strong>Adam</strong> 등등 많은 Optimization algorithm들이 존재합니다. 이 optimization기법의 원리는 <a href=\"https://https://gunu9441.github.io/AI/fancier-optimization/\">여기</a>에 기술되어 있으니 참고하시면 좋습니다😉</p>\n<p>    하지만 이런 algorithm과 함께 동작하는 hyperparameter가 존재하는데요?! 바로 <strong>learning rate</strong>입니다. 이에 좋은 Optimization기법을 사용하는 것도 중요하지만 이와 함께 사용되는 learning rate 또한 중요하겠죠?? 기본적으로 learning rate는 아래와 같은 형태로 사용됩니다.</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi><mo>=</mo><mi>W</mi><mo>−</mo><mi>α</mi><mfrac><mrow><mi>ϑ</mi><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mi>ϑ</mi><mi>W</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">W = W-\\alpha{\\vartheta Loss \\over \\vartheta W}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2251079999999999em;vertical-align:-0.345em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mord\"><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8801079999999999em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">ϑ</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">ϑ</span><span class=\"mord mathdefault mtight\">L</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">s</span><span class=\"mord mathdefault mtight\">s</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<p>    위의 수식은 vanilla gradient descent이며 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span>가 어떻게 update되는지 표현하고 있습니다. <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Loss</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span></span></span></span>를 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span>에 대해 편미분한 값(이것을 <strong>gradient</strong>라고 부르죠??👍)에 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>를 곱하고 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span>에 빼주어 update합니다. 여기서 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>가 <strong>learning rate</strong>를 의미하는데 이 learning rate는 수식에서 볼 수 있듯이 update될 gradient의 비율을 결정하게 됩니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 703px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 82%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACvElEQVQ4y41U2XbaMBDl/z+iTz196+lTlyQ0hJiGNeBiB2MDxhvINt5ttmBzK0RCQtKHzDn3eCSNruaORi5xHIdn21Ns1wnSYMzGRZE/oXjl52fz+/3+af3ol7rdLi4vL9HrdRnJernAKrbwEaP7z0gZ4WAwQJamSCkOFvsjpKEK36ghcvtw1TKCWQuexsHT/8AzOEQOD3d8gd0mYqrOMhTFh9OJiT9BRsk26xSRLVLfQDjn6byK2B0i8UYIiYDUH9PxALvt8pTlifDbl0/gGxwNVrDJZnRhj4/afyX/+PoZcquC4VBDEK2OsuMMjhtQeCAkpPARBhFs4sHzYnZ9RZ6/Izt8S9c3ZWhihxGNlDlmlgfTcFCtDnHLSbj4JVJfRrksgOOGaDZUFpvnzzKLc8m8yIOv/8ZjGrNAxw4wHs2pt3sr8OTl+bFt8qfWyV+1UEl8EKCaIsiwj2J/3JQmSzyIBuIoY+PdLv9QbZlkRVbghjNMpR6WhLws5nuaqQ3XPma+L7aY6AMY8wn0+RjqTMaUgixMKJoIL3JYXEmSJJbqIejv3RWIMUK2yZBkEbJVDFmZosfL0I0Zap0r3EscOgMO9f4Nat0y2gKHntyA7R8fA2tslhEt7pr2lXJfhyK24MY25p6JRWhCs1QM5CkEUQXfl6CZBnyaUZw6CGKCMHHg+CZMmj3LkBV6tzvVPdBUhPr0UJR3dcqyLSwrxHTiQtcWWNA2IjZtp4WP7Wb1QvjW1nEEbzJGYltIXYJVFLOiP5+aFwdFO9iLlCIDcRJoekBvWRAwb7dg1Gsw6YsxuAqMWhXkvolZuw7153ckxEJkUZn0gAOCiYxYp8/UnuEx9PAYedgE9KfieUfCfvUGAiWUWncQbiuQ2g306XfQrENqNkFcB3ObUKnWEaYJfapiJImQ+S4UCqnbQatyjX9EOrK6kn0X6QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 09 03 1\" title=\"assets 2021 09 03 1\" src=\"/static/a1ebf27dc21d3bda76e778b2cd3d86b1/242e2/1.png\" srcset=\"/static/a1ebf27dc21d3bda76e778b2cd3d86b1/5a46d/1.png 300w,\n/static/a1ebf27dc21d3bda76e778b2cd3d86b1/0a47e/1.png 600w,\n/static/a1ebf27dc21d3bda76e778b2cd3d86b1/242e2/1.png 703w\" sizes=\"(max-width: 703px) 100vw, 703px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    위의 그림을 보게되면 <strong>learning rate의 정도</strong>에 따라 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Loss</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span></span></span></span>가 어떻게 떨어지는지를 볼 수 있습니다. 이렇게 update 속도를 결정하고 있는 learning rate는 우리가 정해야할 <strong>가장 중요한 hyperparamter</strong>라고 할 수 있습니다. <strong>Random search</strong>나 <strong>다른 방법들을 이용해서 고정된 learning rate의 최적값</strong>을 찾을 수 있지만 우리는 learning rate decay를 사용하여 좀 더 효율적으로 학습을 진행시킬 수 있습니다.</p>\n<h2 id=\"learning-rate-decay\" style=\"position:relative;\"><a href=\"#learning-rate-decay\" aria-label=\"learning rate decay permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Learning Rate Decay</h2>\n<p>    <strong>Learning rate decay</strong>란 <strong>학습이 진행될수록 learning rate를 점점 더 낮추는 것</strong>입니다. 왜 학습이 진행될 수록 learning rate를 점점 더 낮추어가는지 아래의 그림을 통해 알아보도록 하겠습니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 478px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 66.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABHklEQVQ4y51Uh4qFMBD0/z9RsGLBGgv2snezsJLT+O64gSFEN5PZdd6zlFLU9z21bUvjOBJwnifzN5hqrCRJKM9zCsOQIK4LBkFAjuNQFEXkeR75vkeu6/LzdV2NolbTNFwsOI6Di7BWVUV1XfMK4kLsy7Lk96+CcRwz79j33djqtm2X0EMQt8G+UvV3Sz6laUpd17EbrLprOTxN07tDtIICOTgMA88UxLzkgIgC8zx/FkSBCXAPx3f8WVDako8CZFnGCcCKaC3Lwl3ogrroQ1DPmOwhArGiKAgxs237USd8dfgp4BCGYxN+fJT7bfcRCAHEDKOAcxBjQJyu2PwHGAXOg3CMmHGw4RIbue1OvBP2fXfV4j8Av390iGeI2hdSQfadX1CZzgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 09 03 2\" title=\"assets 2021 09 03 2\" src=\"/static/3bfc344eea4778c0e9f171f04e224a6a/50978/2.png\" srcset=\"/static/3bfc344eea4778c0e9f171f04e224a6a/5a46d/2.png 300w,\n/static/3bfc344eea4778c0e9f171f04e224a6a/50978/2.png 478w\" sizes=\"(max-width: 478px) 100vw, 478px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    위의 사진은 Resenet 논문에 나와있는 그림입니다. Loss가 특정 지점에서 수렴하는 듯 하더니 <strong>learning rate을 낮추니까 Loss가 떨어지는 것</strong>을 볼 수 있습니다. Loss가 일정 지점에서 수렴하는 이유는 <strong>learning rate가 높아 global minimum쪽으로 더 깊게 들어가질 못하기 때문</strong>입니다. 이 상황에서 <strong>learning rate를 줄여준다</strong>면 <strong>update되는 정도가 작지만 정밀하기 때문에 지속해서 global minium 쪽으로 내려갈 수</strong> 있습니다. 위의 그림을 좀더 입체적으로 보자면 아래의 그림처럼 나타낼 수 있습니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 611px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 43.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABOklEQVQoz11S7W6DMAzs+7/Q3mD/+2/dNE1FailDQCkhIR9wyxnMqlpyTRz7cvb1sCwLaIwxRkzTJD6OI4wxEvWb7pyDz/chBMzzDDXFOfCHlywkEKO1VnIrsJWzzfk9l8/e+/3xZ1IHTaaUcnSYc2QjWaSYWaSI6CPmmKTO2BHV0GCwRuo4FR/cGbKIJqxyU/cYMRibGzwu9R3f1wbH8wVfVYniXuLXtCjaEi56YUlAXZUAkhk9+Ak+LSjKBnXvUHUGp3ON91OBt48jPssrHuMgTToRWXFFGgWQC6e1vcHPrYPz6z45Lhk0PRlPiCHm3MqEeQqio/JM3xk6Z9Ebh7Lp4TYF/5fuxEXdrVFFVJF0XBFFP4IUenlVlaZT0V3p7Y5TPYupGAKo6tA4RtpU1oZXJ0PWvP4HFecPXRm96A6riwYAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 09 03 3\" title=\"assets 2021 09 03 3\" src=\"/static/ed859487cbcefb307eeea7953a7db9bb/36bb5/3.png\" srcset=\"/static/ed859487cbcefb307eeea7953a7db9bb/5a46d/3.png 300w,\n/static/ed859487cbcefb307eeea7953a7db9bb/0a47e/3.png 600w,\n/static/ed859487cbcefb307eeea7953a7db9bb/36bb5/3.png 611w\" sizes=\"(max-width: 611px) 100vw, 611px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    파란색 선을 먼저 보겠습니다. <strong>small mini-batch</strong> 단위로 <strong>mini-batch gradient descent를 활용</strong>하게되면 <strong>노이즈</strong>를 갖고 minimum을 향해 가게 됩니다. 하지만 minimum 값에 정확하게 수렴하지 않고 그 주변을 돌아다니게 됩니다. 왜냐하면 <strong>고정된 learning rate를 사용</strong>하고 있고 <strong>서로 다른 미니배치들에 노이즈가 존재</strong>하기 때문입니다. 이 때, <strong>learning rate</strong>를 천천히 줄이게 되면 초록색 선처럼 <strong>초기 단계에서는 빠르게 학습</strong>하다가 점점 step이 작아지면서 <strong>minimum에 제대로 수렴</strong>할 수 있게 됩니다.</p>\n<p>    하지만 <strong>learning rate decay는 Adam을 사용할 때보다 M-SGD(SGD Momentum)을 사용할 떄 더 많이 사용</strong>하며 부차적인 하라미터입니다. 일반적으로 학습 초기부터 learning rate decay를 고려하지 않습니다. <strong>처음엔 learning rate decay를 생각하지않고</strong> learning rate를 잘 선택하는 것이 중요합니다.</p>\n<p>    따라서 learning rate decay를 설정하는 순서는 아래와 같습니다.</p>\n<ol>\n<li><strong>decay없이 학습을 실행</strong>합니다.</li>\n<li>Loss curve를 살피고 <strong>decay가 필요한 곳</strong>이 어디인지 고려해봅니다.</li>\n<li><strong>learning rate decay를 적용</strong>합니다.</li>\n</ol>\n<h2 id=\"the-kinds-of-learning-rate-decay\" style=\"position:relative;\"><a href=\"#the-kinds-of-learning-rate-decay\" aria-label=\"the kinds of learning rate decay permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The Kinds of Learning Rate Decay</h2>\n<p>Learning rate decay는 다양한 방법들이 존재합니다.</p>\n<p>① <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>y</mi><mtext> </mtext><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>∗</mo><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mtext> </mtext><mi>n</mi><mi>u</mi><mi>m</mi></mrow></mfrac><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha ={1\\over{1+decay~rate * epoch~num}}\\alpha_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.326216em;vertical-align:-0.481108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mbin mtight\">+</span><span class=\"mord mathdefault mtight\">d</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">c</span><span class=\"mord mathdefault mtight\">a</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace nobreak mtight\"><span class=\"mtight\"> </span></span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">a</span><span class=\"mord mathdefault mtight\">t</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mbin mtight\">∗</span><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">p</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">c</span><span class=\"mord mathdefault mtight\">h</span><span class=\"mspace nobreak mtight\"><span class=\"mtight\"> </span></span><span class=\"mord mathdefault mtight\">n</span><span class=\"mord mathdefault mtight\">u</span><span class=\"mord mathdefault mtight\">m</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.481108em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></p>\n<p>①은 epoch가 지날 때마다 분모가 커지면서 초기 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>의 값에 곱하여 learning rate를 작은 값으로 만들어 주게 됩니다.</p>\n<p>② Exponential decay</p>\n<p>Exponential decay는 지수형태로 되어있는 decay로 아래와 같은 수식을 갖습니다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.9</mn><msup><mn>5</mn><mrow><mi>e</mi><mi>p</mi><mi>o</mi><mi>c</mi><mi>h</mi><mtext> </mtext><mi>n</mi><mi>u</mi><mi>m</mi></mrow></msup><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.95^{epoch~num}\\alpha_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.049108em;vertical-align:-0.15em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span><span class=\"mord\"><span class=\"mord\">5</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991079999999999em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">e</span><span class=\"mord mathdefault mtight\">p</span><span class=\"mord mathdefault mtight\">o</span><span class=\"mord mathdefault mtight\">c</span><span class=\"mord mathdefault mtight\">h</span><span class=\"mspace nobreak mtight\"><span class=\"mtight\"> </span></span><span class=\"mord mathdefault mtight\">n</span><span class=\"mord mathdefault mtight\">u</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>    epoch가 지날 때마다 0.95가 계속 곱해지면서 초기 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>값인 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>가 계속해서 작아집니다.</p>\n<p>③ Step decay</p>\n<p>    epoch가 지날 때마다 계단처럼 learning rate를 감소시킵니다. 예를 들어, 20epoch가 지날때마다 0.1을 곱하여 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>값을 작게 만듭니다.</p>\n<p>위에 존재하는 수식에서 hyperparameter는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>y</mi><mtext> </mtext><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">decay~rate</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace nobreak\"> </span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\">e</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>α</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>입니다.</p>\n<h2 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Summary</h2>\n<ol>\n<li>Learning Rate Decay를 사용하게 되면 <strong>학습 알고리즘의 속도를 높일 수 있습니다</strong>.</li>\n<li><strong>좋은 Learning rate를 선정한 이후</strong>, Loss curve를 보고 <strong>decay가 필요하다고 느껴지면 적용</strong>합니다. 따라서 <strong>다른 것 보다는 우선 순위가 낮습니다</strong>.</li>\n</ol>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li>learning rate decay: <a href=\"https://velog.io/@good159897/Learning-rate-Decay%EC%9D%98-%EC%A2%85%EB%A5%98\">https://velog.io/@good159897/Learning-rate-Decay의-종류</a></li>\n<li>learning rate decay(including C2W2L09 Lecture): <a href=\"https://gaussian37.github.io/dl-dlai-learning_rate_decay/\">https://gaussian37.github.io/dl-dlai-learning<em>rate</em>decay/</a></li>\n<li>사진 인용 및 설명: C2W2L09 Lecture</li>\n</ul>","frontmatter":{"title":"Learning Rate Decay","date":"September 03, 2021"}}},"pageContext":{"slug":"/AI/learning-rate-decay/","previous":{"fields":{"slug":"/AI/fancier-optimization/"},"frontmatter":{"title":"Fancier Optimization"}},"next":null}},"staticQueryHashes":["2486386679","3128451518"]}