{"componentChunkName":"component---src-templates-blog-post-js","path":"/AI/batch-normalization/","result":{"data":{"site":{"siteMetadata":{"title":"Gunu's AI Log","author":"[Gunu]","siteUrl":"https://gunu441.github.io","comment":{"disqusShortName":"","utterances":"gunu9441/gunu9441.github.io"},"sponsor":{"buyMeACoffeeId":"gunu9441"}}},"markdownRemark":{"id":"e58a722a-a221-55ec-be59-a944862c85a9","excerpt":"이번 시간에는 Batch Normalization에 대해서 알아보도록 하겠습니다!👏 이전 포스트에서 Xavier initialization이나 He initialization이 무엇인지, 어떤 원리를 통해 만들어 졌는지 알아보았습니다. 이에 우리는 이 2개의 initialization 기법이 각각의 layer에서 나오는 각각의 activation value의 distribution…","html":"<p>    이번 시간에는 Batch Normalization에 대해서 알아보도록 하겠습니다!👏 이전 포스트에서 Xavier initialization이나 He initialization이 무엇인지, 어떤 원리를 통해 만들어 졌는지 알아보았습니다. 이에 우리는 <strong>이 2개의 initialization 기법이 각각의 layer에서 나오는 각각의 activation value의 distribution을 같은 개형</strong>으로, <strong>좀 더 가우시안스럽게</strong> 만들기 위해 사용했습니다. 하지만 이렇게 모든 레이어에서 나오는 <strong>activation value들의 distribution을 gaussian처럼 유지</strong>시키는 또 다른 아이디어에는 <strong>Batch Normalization</strong>이라는 기법이 있습니다. 아래는 이번 시간에 배울 Category입니다.</p>\n<h2 id=\"category\" style=\"position:relative;\"><a href=\"#category\" aria-label=\"category permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Category</h2>\n<ol>\n<li>Internal Covariate Shift</li>\n<li>Learnable Parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span></li>\n<li>Effect of Batch Normalization</li>\n<li>Batch Normalization operation</li>\n<li>Code</li>\n<li>Summary</li>\n<li>Reference</li>\n</ol>\n<h2 id=\"internal-covariate-shift\" style=\"position:relative;\"><a href=\"#internal-covariate-shift\" aria-label=\"internal covariate shift permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Internal Covariate Shift</h2>\n<p>    우리가 어떤 네트워크에 넣게 되는 input이 정규 분포의 형태를 가졌다고 생각해봅시다. 이 입력이 레이어를 통과할 때마다 아래의 그림처럼 <strong>activation value의 distribution 흐트러지게 됩니다</strong>.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 722px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 20.666666666666668%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAmElEQVQY01VPWxIDIQjb+1+2dRYFUUiB7vaRDyeGKMkBONwB24aE+/ueunmeJX61EIte3kRpxR1HClsXRutBdplvI5PU7PcxT8OS+beEp+PKkx/GVhGIxlQFlyv3gs9Rsw/cIMQw5uI3RA0eYaImDp0LRIo2HKNPrLWr6oh0jTbonJiyKsE4Ga1b+TU+yarZ4vlgUGNIF7wAgu87tIQzEoYAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 1\" title=\"assets 2021 08 23 1\" src=\"/static/3410a6c5c91fe08b1262fa1ba0328c2b/d44c9/1.png\" srcset=\"/static/3410a6c5c91fe08b1262fa1ba0328c2b/5a46d/1.png 300w,\n/static/3410a6c5c91fe08b1262fa1ba0328c2b/0a47e/1.png 600w,\n/static/3410a6c5c91fe08b1262fa1ba0328c2b/d44c9/1.png 722w\" sizes=\"(max-width: 722px) 100vw, 722px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    layer가 깊어질 수록 위의 그림처럼 activation value의 distribution이 점점 심하게 흐트러지게 되는데우리는 이 현상을 internal covariate shift라고 부르며 이를 해결하기 위해 batch normalization을 사용합니다. batch normalization을 사용하여 각 레이어의 activation value를 강제적으로 gaussian 분포로 바꿔보자는 것입니다. 즉, forward pass하는 동안 각각의 레이어로부터 나온 <strong>Batch 단위 만큼의 activation들이 가우시안 분포</strong>이길 바라는 것입니다.</p>\n<p>    Batch Normalization은 <strong>레이어 안에 있는 각각의 뉴런들을(같은 feature끼리)</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>(mean)와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 통해 <strong>normalization</strong>합니다. 예를 들어 batch 당 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span>개의 학습 데이터를 갖고 있고 각각의 학습 데이터가 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span>차원이라 할 경우, batch당 input의 크기는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>∗</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">N*D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">∗</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span>가됩니다. 아래의 그림의 경우 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10903em;\">N</span></span></span></span>이 50, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span>는 150이 되며, 레이어에 <strong><em>150-dimensional input</em></strong>이 들어가는 것 입니다. <strong>데이터의 같은 dimension끼리 normalization을 수행</strong>하기 위해 아래의 그림처럼 같은 feature끼리 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>μ</mi><mi>n</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mu_{n}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>σ</mi><mi>n</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\sigma_n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 구해줍니다. 이후, 구한 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>μ</mi><mi>n</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mu_{n}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>σ</mi><mi>n</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\sigma_n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 통해 <strong>feature 당 normalization을 진행</strong>합니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 870px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 63%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfUlEQVQ4y4VTgXKDIAz1/79xu7VatVorqCgiYPZCy6a71uUuB5L05b0kTZqmoaIo6Hq90jiOxLZ6T9HinWNSysfbur715FbXlKUZVVVFalA/QPOykDbm55sL53lO/1ny6lHUFZWfHzS3dxqFIAFmXdfRMAyBZXR+5/giBfVQqHBPtnTZPCS2l4xsJ4Mz6AKmCxib5xmdv2fR0oKc+Y7ize0XMII558gOPblpJIe+ObA6MqcG5A84FRkQSGKT2SrQLssygFj1SLJ9fzAEH4rbvgvnDOk7QGbHd4OAZTCujuS3gFDEBQNgvwHcgnKSQbO9noIzyyPziLunmh3DCGjB8n76oinPSGVnEucTptmRxAS3E5bP6UvEFfLH9Ewa27Fbmwi6QIJpMT2eMqZowXo73ejWr6QxWc7nXJa9kxwBza0OQd4vnZ4OJWuo0FlKGoomsH0NiKphdUb0pSze/t0CYH4JQDPkauzvS8lOa/KQFH6Inm5jf4uv1j5yvQv3b2NN90S+HWnBAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 2\" title=\"assets 2021 08 23 2\" src=\"/static/a1f6fd4f9152712d0c73c7e9f7ceaf65/3f3b9/2.png\" srcset=\"/static/a1f6fd4f9152712d0c73c7e9f7ceaf65/5a46d/2.png 300w,\n/static/a1f6fd4f9152712d0c73c7e9f7ceaf65/0a47e/2.png 600w,\n/static/a1f6fd4f9152712d0c73c7e9f7ceaf65/3f3b9/2.png 870w\" sizes=\"(max-width: 870px) 100vw, 870px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p align=\"center\">\n    150-dimensional input\n</p>\n<p>    Batch normalization layer는 <strong>fully-connected layer 이후나 Conv layer 이후</strong>에 바로 넣어주게 되며 <strong>activation function이 전</strong>에 넣어줍니다. 만약 conv layer를 batch normalization 시킨다면 <strong>batch normalization을 channel 단위로 수행</strong>하여 줍니다. 이에, <strong>activation map 마다 평균과 분산</strong>을 구하여 normalization 시켜주는 것입니다.</p>\n<h2 id=\"learnable-parameters-span-classkatexspan-classkatex-mathmlmath-xmlnshttpwwww3org1998mathmathmlsemanticsmrowmiγmimrowannotation-encodingapplicationx-texgammaannotationsemanticsmathspanspan-classkatex-html-aria-hiddentruespan-classbasespan-classstrut-styleheight0625emvertical-align-019444emspanspan-classmord-mathdefault-stylemargin-right005556emγspanspanspanspan-and-span-classkatexspan-classkatex-mathmlmath-xmlnshttpwwww3org1998mathmathmlsemanticsmrowmiαmimrowannotation-encodingapplicationx-texalphaannotationsemanticsmathspanspan-classkatex-html-aria-hiddentruespan-classbasespan-classstrut-styleheight043056emvertical-align0emspanspan-classmord-mathdefault-stylemargin-right00037emαspanspanspanspan\" style=\"position:relative;\"><a href=\"#learnable-parameters-span-classkatexspan-classkatex-mathmlmath-xmlnshttpwwww3org1998mathmathmlsemanticsmrowmi%CE%B3mimrowannotation-encodingapplicationx-texgammaannotationsemanticsmathspanspan-classkatex-html-aria-hiddentruespan-classbasespan-classstrut-styleheight0625emvertical-align-019444emspanspan-classmord-mathdefault-stylemargin-right005556em%CE%B3spanspanspanspan-and-span-classkatexspan-classkatex-mathmlmath-xmlnshttpwwww3org1998mathmathmlsemanticsmrowmi%CE%B1mimrowannotation-encodingapplicationx-texalphaannotationsemanticsmathspanspan-classkatex-html-aria-hiddentruespan-classbasespan-classstrut-styleheight043056emvertical-align0emspanspan-classmord-mathdefault-stylemargin-right00037em%CE%B1spanspanspanspan\" aria-label=\"learnable parameters span classkatexspan classkatex mathmlmath xmlnshttpwwww3org1998mathmathmlsemanticsmrowmiγmimrowannotation encodingapplicationx texgammaannotationsemanticsmathspanspan classkatex html aria hiddentruespan classbasespan classstrut styleheight0625emvertical align 019444emspanspan classmord mathdefault stylemargin right005556emγspanspanspanspan and span classkatexspan classkatex mathmlmath xmlnshttpwwww3org1998mathmathmlsemanticsmrowmiαmimrowannotation encodingapplicationx texalphaannotationsemanticsmathspanspan classkatex html aria hiddentruespan classbasespan classstrut styleheight043056emvertical align0emspanspan classmord mathdefault stylemargin right00037emαspanspanspanspan permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Learnable Parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span></h2>\n<p>    하지만 batch normalization은 단순히 normalization만을 수행하는 것은 아닙니다. 아래의 그림과 같은 수식을 통해 <strong>normalize된 distribution을 sclaing 하거나 shifting</strong>시킬 수 있습니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 409px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 82%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACKElEQVQ4y4WU2ZKiUAyGef8Xmr7xxp7yznJfUBEUV1rBBXE3c77YpwdnempSleIQTpI/fxKcwPelUCiI53kyGgVSLpf1PJtNZTgcijf0pNvtSrvdllarpc8wDGW325k7M9nv9xJFS7nf75JlmTibzUaOx6OqNxhIvV6XRqMh1WpFgiCQXq+n7wRar9eSJIlcLhdBHo+H3G43DYYcDgdxcCoWi/L+810Wi4Vqmqbiuq4sl0uZz+eKwApBUILkzwh+DsjywjvQkbxTXv8l+GpAyoiiSA3wSJlkyyPKB+MuPPuGfyoZjcfqQ3UOaFACwMF0OtFmnM/nlyD5M4m32602hB7YZ5JsxCFIXrRrH9GL7Xq9fp0tX9+JdhkklOD7Q80cBL7C/zBBSQbaOIm/nGhUv983dk+azaaOEGNF6VqyRWjnCCOXGAf0dDq9oOAeYwNqi9xSkaaH3xzCCc4gGBuSw3D816j82eFDBudTBTCZhLJarZ4BKZsniFy3qyUQ+DvJB73fbzrsaxMojmOdFg0IMhButxuTafK5EbGOBdkTcxmbLZHEcEwlJJ4aH98sCGfHcgRKOxLwhBP80nX0eHpdAO6DaJ/uda8BpAhZLZa+0+nozsIJGRlcvj35mWhXWdOB2XeU7oMIm+cNFC3T4GB8e/uh+1wqlaTX76nWalWjNalUKmrn50AC2zSSPMfNN9s1Mt9mkh2z59gAFVLt9MMfZRDArqWl5H/yC9wGwBd8ZIxfAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 3\" title=\"assets 2021 08 23 3\" src=\"/static/a220a37193b07ab2006bfbe1d72d0473/7a75e/3.png\" srcset=\"/static/a220a37193b07ab2006bfbe1d72d0473/5a46d/3.png 300w,\n/static/a220a37193b07ab2006bfbe1d72d0473/7a75e/3.png 409w\" sizes=\"(max-width: 409px) 100vw, 409px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    <strong>batch normalization을 통해 gaussian으로 normalize된 값</strong>들을 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>로 <strong>scaling</strong>을, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>로 <strong>shifting</strong>을 할 수 있습니다. <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>는 <strong>learnable한 parameter</strong>이며 <strong>normalize된 distribution을 flexible</strong>하게 만들어 줍니다. 이에 normalized 된 값을 다시 원상복구시키고 싶다면 원상복구 할 수 있게 됩니다. <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>를 <strong>분산으로</strong>, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>를 <strong>평균으로</strong> 설정하게 된다면 말이죠~!👍 이렇게 감마와 베타를 사용하면 <strong>saturation을 얼마나 할 것인지 학습</strong>할 수 있기 때문에 학습에 대한 유연성을 얻을 수 있습니다.</p>\n<h2 id=\"effect-of-batch-normalization\" style=\"position:relative;\"><a href=\"#effect-of-batch-normalization\" aria-label=\"effect of batch normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Effect of Batch Normalization</h2>\n<p>    Batch normalization을 사용한다면 전에 설명했던 것 처럼, 각각의 layer를 통과한 output을 <strong>Gaussian스럽게</strong> 바꿀 수 있습니다. <strong>강제적으로 Gaussian distribution에 근사하는 activation value들의 distribution</strong>을 얻음으로써 <strong>각각의 layer를 통과한 activation value들이 서로 비슷한 distribution을</strong> 갖게 됩니다. 물론 모든 분포가 gaussian distribution을 따를 수는 없지만 <strong>gaussian distribution에 근사하도록 normalize</strong>시키게 됩니다. 이에 <strong>internal covariate shift가 없어</strong>지게 되는 것 입니다.</p>\n<p>    또한 batch normalization은 <strong>regularization의 역할 또한 수행</strong>합니다. 레이어의 출력이 해당 데이터 뿐만아니라 <strong>batch 안에 존재하는 모든 데이터 들의 영향</strong>을 받게 됩니다. <strong>각각의 레이어에 있는 동일한 feature들끼리 평균과 표준편차를 구하여 함께 normalize 되기 때문</strong>입니다.</p>\n<h2 id=\"batch-normalization-operation\" style=\"position:relative;\"><a href=\"#batch-normalization-operation\" aria-label=\"batch normalization operation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Batch Normalization operation</h2>\n<p>    Batch normlization은 <strong>train할 때와 evaluate할 때 다른 동작</strong>을 합니다. 먼저 train 할 때, 이루어지는 동작을 살펴보겠습니다.</p>\n<p>    <strong>train을 할 때</strong>, batch normalization은 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>를 <strong>backpropagation을 통해 학습</strong>하게 됩니다. 아래의 그림처럼 layer안에 있는 node(feature) 하나당 하나의 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>를 가지게 되는데 이는 하나의 node(feature) 당 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 갖기 때문입니다.. 또한, train 과정에 있어서 normalize하는데 사용되는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>는 <strong>batch 단위로 구해지게 됩니다</strong>.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 569px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 36%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABVUlEQVQoz3WR7W6CQBBFff+XMtFoU9SWSlEICAssH5IiLEsBxfSWWVKTpil/CMvM2TN3ZkVRII5jJEmCtm0xPV/oYx+VbUD6NjC06izNMkRRhDRNUZalqqzr+nFGjNkwDMjGQt8fAVWFruvQfUqkzEXoufAdG8G7DnHOcKkEojAEYwxCCPR9Dyml6iWp6/WK2f1+R57ncBwHm80GnufheDChrZeIAoaUByi5j+bygXpspkbbtrHb7RSI3qZpKsPb7TYB6YN+7vd7LJZLPK1XOL69IPBPYKMh9x3IWozjSYSjIV2q6zrm8zm22y1c1wXn/C/QMIwH0DJewbwTsoihOnM0I1DK5l8g5aiAlOHPyJqmKbB1OOB5tQAP2ZihhehkoRalyo1MLMtSo1KWBCQRkvqVId1MDdNSGpwjDyHzkCchmiKDFJVaGgHJhpZBtU0zWdOGSe4bWkwCN5rjE5kAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 4\" title=\"assets 2021 08 23 4\" src=\"/static/87b9865c3efa940b789ef0025359a7de/854dc/4.png\" srcset=\"/static/87b9865c3efa940b789ef0025359a7de/5a46d/4.png 300w,\n/static/87b9865c3efa940b789ef0025359a7de/854dc/4.png 569w\" sizes=\"(max-width: 569px) 100vw, 569px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    <strong>evaluation 과정</strong>에서 사용되는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>는 아래의 그림처럼 <strong>train 과정에 구했던</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>들의 <strong>각각의 평균 값이 사용</strong>됩니다. <strong>train 과정</strong>에서는 들어오는 batch를 기준으로 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 구해서 <strong>매 step마다 값이 바뀌었습니다</strong>. 하지만 <strong>evaluation 과정에서는 train에서 구했던</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>들 <strong>각각의 평균 값을 사용하므로 고정된 값을 사용</strong>합니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 898px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 27%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABJElEQVQY0y2Q2W6CYBCFef836U0vmvQFmjSNQa2KghugAcStLCLwo8DXgXaSk8zFydm0LMv4Ho/xPI88z4njhOnMwLYdojgmOBwwDIPr9UoYhri7HUmSMB+NcLZb3NUK3/dRStG2LZqqKuamyVAIaZqSishy8Em4WhLt9vjTKXsRLC4XyrLs8Xw+uQcBpUCJSVXX1IJesJGEuC5qvaKy1jw2nzTOC21ypC0UEpvuOvL/Q/epzYZC1ymmM5RloSR5JQZaIam8xQJ1PpN6PvnJ5ZHY1GXeu/85N32qrlYljRoRvFsmuW2TOzZVdqOuHjTC0bqddNmwEHKUpOTlnYnxwWjwytH/Qh++Y8zeiCKTIAi53VJ+oojwMGG333A6jqSgTTddd7/OFnVRS/2LVAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 5\" title=\"assets 2021 08 23 5\" src=\"/static/b411e4e9267ac1c92fbead6e65c3fba1/84cc5/5.png\" srcset=\"/static/b411e4e9267ac1c92fbead6e65c3fba1/5a46d/5.png 300w,\n/static/b411e4e9267ac1c92fbead6e65c3fba1/0a47e/5.png 600w,\n/static/b411e4e9267ac1c92fbead6e65c3fba1/84cc5/5.png 898w\" sizes=\"(max-width: 898px) 100vw, 898px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    evaluation(test) 할 때는 아래와 같은 과정이 진행되게 됩니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 631px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 58.333333333333336%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACLklEQVQoz3VTa2+iUBDl/3/2R1gTE6N21xq/dNvGNI2xWlvjC6soPgAfVVDwwukZFndJNnuTAe7cO2fmnBm0breL5XIJWWEYIoqi+H21KLFQ/EpBeSdc1mv4iyW/PYSMiyQ2Ma3X62HNC7JO5zMul8tfMIJ4QQCVgAVfXwgJ4rIAdzyG2u2gfB8X+pXrIrAsaEiWfzxiPp3Csu2rCwEvj19eYJlmDBQyWcBEMwKahgF/MkHgODh/fsJ7e8OZe00qkrXlgcFLjUYD7XYbO2b/dX+PzuMj9GYTHi+rzQYOGY0ZPGZyl/tgNsPh4QHex0dMXRNtYsDtFnYCWq1W0Xx9xY9yGfPnZ8xHIxxJSZZNeRxSWywW2B0OiE4n+MKAkohMmhJHAubQ3pj95iaHMsFyuRz6rMjUdVj9PkJnA4tNNGiDkY49WeDalER37bLf48wMNkFdViES1Go1ZDIZtFothGyKy0rkXJZFmj59B/o2/E5PRAworVa09WqNIxujs5p8Po9sNotSqYTJ0xNsUrYofEQQi901qadDsC1NkWp61DR5iFMoy/gMh0MUi0XU63UUCgXMBgM4FN6azzkKPmy+pcOi4Z7s0nR/a6jUny7LgHc6HVQqFUwZ9PP2Fp27Oyze37GjLBElsVmhwSQz7gUQ/wPcc/5MZtcZIMByKB03WbVUdCK9iLN44N4g/RXfouM/gOm/QijbBJb99eKS1DZsSPr3EnlWq1UcI1MSpQC/AbhZh4tfmlYCAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 6\" title=\"assets 2021 08 23 6\" src=\"/static/1b522ddf26f89bf6280f8142519c9db5/4597d/6.png\" srcset=\"/static/1b522ddf26f89bf6280f8142519c9db5/5a46d/6.png 300w,\n/static/1b522ddf26f89bf6280f8142519c9db5/0a47e/6.png 600w,\n/static/1b522ddf26f89bf6280f8142519c9db5/4597d/6.png 631w\" sizes=\"(max-width: 631px) 100vw, 631px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<h2 id=\"code\" style=\"position:relative;\"><a href=\"#code\" aria-label=\"code permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Code</h2>\n<p>    batch normalization의 성능을 시험해 보기 위해 <strong>batchnormalization, initialization을 적용한 모델</strong>과 <strong>initialization만을 적용한 모델</strong>을 training 하여 비교해 보았습니다. 처음에 코딩을 했을 때, <code class=\"language-text\">model.train()</code>과 <code class=\"language-text\">model.eval()</code>을 사용하지 않았더니 generator가 아래와 같은 사진을 생성해 냈습니다.</p>\n<p align=\"center\">\n    &lt; batchnorm과 initialization 적용한 model &gt;\n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 243px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 150.2057613168724%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAYAAAAsEj5rAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAD1ElEQVRIx03W5W4mOwwG4NxcQVVVZu6WmZmZcVv1crN6fOTq/BjNV8fxC3YyLXNzc3V/f7+OjIzU7e3tur6+XqenpyM2Oztbd3Z26szMTF1dXY11sdPT07qwsFBXVlbq/Px8/fPnT+Q9PDzUsrm5Wa+ururU1FS9v7+PTUtLS/X8/LwuLi7Wl5eX+L2xsVFvb2/jfXFxEYC7u7v18fExQK6vr+vb21st4+PjUWBoaCgW+vr66sDAQKB3dnbWvb29YEiB2ODgYIBiNTExEWrEtra2onhB/fDwsE5OTtbj4+N41tbWYhM7rGGSzMSwWV5ertRhSrZ97+/v/zEkhTwoEoEcHBzU0dHRYCiGiaL9/f0BSJV8BYHw+Obmppbh4eFIIElDFCaByaTzVhFvBYDYjJV9WOYaBQVVZkrClE8Kn52dBbOPj4+Qw4LPz89gRrJGAc2GUaSpBQtJra2tUZSXmgCxq6srimKlAX6LHR0dRVOyMd3d3cEYiYKR+cEqx4EUaNawUfzk5CR+89dGBTDEDCHqvr6+amGooAT+GR2P7o6NjUWHbVQcQzG/+aa4GU72IZkcm3QbEwCkAGBHdp0ClshHQBMVt5anjJ/FBlRtIOny8jIk3d3dRczfGAIlW76jx2tvNgHiaxy9nHJvzfAYJTFMUyYWvHWi5ChonSLqEqBoOTYkMpZ3NhpSZkM13Jr19PT02yiMxK0DxfT5+bkWhioKwVuxHFRsbPLO20YeD60DFFcDAMCSm82SzWRgISHN1n0eyiOXClIV0hQxflNYFKGdD6SQZlHM+c0Tg7l1YArm6SA5L5G/f//W4oRY1JSctf83IJuiUXmuqbCmkPWenp5QEQxJgoK6RkBVTKMAuOPIlSdGtjw5cimRpynWC3QMoUoyuJD9NtiQme+tqHX5LmLeYY09Qma2ZCfzgnUdKQ7RWxGdtAGQIslKMX4rbj0kk+OmTekkYeBKk+R05McqrzfnFzjJHutscNX9XrAYKqDbKYE8QGzxkO47I65xpgBrv1kSN7Z2Q4SMDURsXKZm0fTLscYGXTXA/GIP1tmcn5+fWtB17Hp7e0Mq1LzeMVHcSIl5xBTDjAoWYQjIJzc+UpgoDDHn0QiwIb9qmpGNki+WH6q8TOIzmidA0JvZCrossVCEJN30zrOMXf634A04xiZPRUdHx++N7TSQD0RTnFkxdohh5uxjbW9bW1usxQXL8LyCdIlsxfiRp8Ibq+/v72CKPSUYaVqO0uvray2+Yujn1y9ZkKABkqloaWmJxjU0NMTotLe318bGxtrc3Fybmpp+Vf0DUwxkeWm/FHQAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 7\" title=\"assets 2021 08 23 7\" src=\"/static/7c078d2fc7b87b89dc22662a4698a152/8aea6/7.png\" srcset=\"/static/7c078d2fc7b87b89dc22662a4698a152/8aea6/7.png 243w\" sizes=\"(max-width: 243px) 100vw, 243px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p align=\"center\">\n    &lt; initialization만 적용한 model &gt;\n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 243px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 160.082304526749%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAYAAAASYli2AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEX0lEQVRIx4WWR09rQQyFLy2UhA4JvfdeRO81oTcpASREE4INCLFhx4oFSKz5u37vO8ERm6e3iObi8XiOz7E9BO3t7dbX12dBEFhzc7MVFRVZa2urraysWCgUspGREcvJybH6+npbW1uT3+bmptXU1FgkEpE9OzvbJiYmbH9/3wI2Zmdndaizs1MHJicn7eHhQd+Li4tWXFxs09PTdnBwIBt7+Dc1NRmAcnNzLZFI2Pr6OvuBlZSUyKGystLy8vL0ze3sFRYWCjU2vrHxd0FBgZCxspeVlaWshHBmZkYGVoL+Tu/8/Fy2ioqKTAaHh4cKenJyYpeXlwqYSqXSGeBYXl4ux+rqagvlhcRdLBaTra2tTasj5zsajWrFh/OkzBloCQgWjyfk0NjYqLWsrExiEMRtODvX7JE+mXAhWe7t7aVF6erqsrOzMx0aGhrKoBofH9c3+3A8PDxsqWRKNhQlYHd3t34AuLi4SMeBVJB4ynDjKD0tyP9NgwuC+lyGjbSlMkSvrq7KeHV1ZXV1dUpFfPy13d/fi+OBgQHtYzs9PdXFY2Njtru7KxtIt7a20mWDiqhcW1tr+fn5IhoE7FFroEMI/LBVVVVpDYfDuhzEIJybm0uLQnQcIJWgdEw8HpcNIfBpaWnJoHH0iMGFjnB7e9sCoPf39wsZqTQ0NKjeIBlHipXbOzo6bGdnRzZUZqVLQE0MvsUhgnhdsemVT9oulJMPv16HpItvT0+PugqfhYUFC7iZysfx8fFRqRH4py/t6elJgehV9rFdX1+rMm5ubiQaCBGWvg+42cn2jsGBA6RKKl5G+CIQ3GGDKi5DUChTL3OY6YJxampKB+GRPvViJyXId17pWe/t3t5epYuI6mVU9XnIJoEphcHBwYyNdXl5WTSAmjajrFCWUuEMM1KiEd1V41b+pq3gksP0MukjAj+vP4Lg6zUMXfAccOvX15duJE04ouJfX191GLJJmU7x2gQNK/0OTQS9vb1NDwdQURo4cJDbSktLM/0NGnjG7n0eCaeHLwgRAxD8oEW9zHDE4e7uTp1CPX1+fmY6BSXhVHX2UzYAgWumkRe5MiCAvxWMH1SmNlEUxOrPv3soT3p8wxWI8CUojUApaR/YBPBaIwirvx9ciA0KvG+Zh1BBx3AWOmLRmB0dHVmAws/Pz3JMJpMaDKOjo3pLuBmiOUxKjCuQfXx8KGWGxdLSkr7JSGWDMyIQ0NffXeM9DWJ+BGR8gYrO8RcTP3FM7txM2TB+OAC57+/vUpX+pCwI4g3gRc9ZfyoIprJBdoh1BzgljePjY6VKp3AJpeXl5c8p/P0eX5kBSyvhAHccIjWfyvPz86ICYdzGI4ZIpOldBpCNjQ0LKIHv72+1GaMKRCCmG0D29vamv/l/B9Q+HOCNLpOyP2BUh9wMAhx80HK7lw2ooMVfNsRgsLpg+Pn7o5QxOrEvLy9CwgHni5s91X/9PKAGLKRSe95mzo+/y7SWI/pfQJT+A3kqWSmOjgViAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 8\" title=\"assets 2021 08 23 8\" src=\"/static/989fc869768eb3039cb7f59b54e4601e/8aea6/8.png\" srcset=\"/static/989fc869768eb3039cb7f59b54e4601e/8aea6/8.png 243w\" sizes=\"(max-width: 243px) 100vw, 243px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    batch normalization은 <strong>train할 때와, test할 때의 동작이 달라</strong> 이를 구분하기 위해서 <code class=\"language-text\">model.train()</code>과 <code class=\"language-text\">model.eval()</code>을 사용하게 됩니다. <code class=\"language-text\">model.train()</code>은 현재 모델이 <strong>training과정을 진행 중</strong>이라는 것을 알려주는 것으로 <strong>batch normalize의 학습이 진행</strong>됩니다. batch normalization의 <strong>학습이 진행</strong> 될 때는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>가 <strong>해당 batch에 따라 다른 값</strong>으로 설정되며 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>가 <strong>학습</strong>됩니다. 하지만 <code class=\"language-text\">model.eval()</code>은 모델이 <strong>현재 test 과정을 진행</strong>하고 있다고 알려주기 때문에 지금까지 구했던 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>들의 <strong>평균을 사용</strong>하여 <strong>고정된</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 사용합니다. 또한 <strong>지금까지 학습한</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span></span></span>를 <strong>사용</strong>하여 결과를 구해줍니다. 아래의 사진은 위의 두 개의 model의 discriminator loss와 generator loss 입니다.</p>\n<p align=\"center\">\n    &lt; Discriminator Loss &gt; \n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 782px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 67.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABTUlEQVQ4y51S25LDIAjN/39nX7OTrjdQkQXUxGSmD93O0KNwPNyyASBXCNxqYaLKrRE3IjuToDhYf20g1c4xNB5xLcjuffC+77yllLhkZMIgwt6QYJzh8pXk+j15bhhP/0QhMIhWFyzVKqhVMgpa1pldzGLUY7UuXGonT2PpEiz9sZC1xdn21T5ZUns4OLTEFcV5F2wju4fMEQv7lDlAt35HOXfU+/RHRbEkZxOMMZpQkRZeu+d3RCNkqWjarF7by7kYN0sRK0d9XVD+lBwkw6+Iaem2RbrabbfZdZwtTx6fM5QKteXXT7BM7bmURejTfSY9Z4i5z2N+e6t9LQiQ2EWwgbeH2L8EEYCPADbUNgK0zO6J9xleXP2BaMmWAx8+MqJsVlrXeSquNn2fUM0WGwJvelBlDcaxIL2rIaKhPnDOWUzRvlfvza9vJl9jfw7b/zFdzRQWAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 9\" title=\"assets 2021 08 23 9\" src=\"/static/8e2ebcbfce978a4015785c754940b08e/2e195/9.png\" srcset=\"/static/8e2ebcbfce978a4015785c754940b08e/5a46d/9.png 300w,\n/static/8e2ebcbfce978a4015785c754940b08e/0a47e/9.png 600w,\n/static/8e2ebcbfce978a4015785c754940b08e/2e195/9.png 782w\" sizes=\"(max-width: 782px) 100vw, 782px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p align=\"center\">\n    &lt; Generator Loss &gt;\n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 778px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 67.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABWUlEQVQ4y4VUCY7DIAzM/z9aqdpuE+7DxmsToCRtukijSeyJGezSxXtPKSWKMQ6EEGrMOUe3243u93vldV2HVjRdb4ylbdtIK0XLZj3FjFQQCRmllAPP65zrnGMgxQV/Hg9afrUjn4DVXJAFnz4CgPqMF5uWtqG4HgVLuRA3/lqwsbRoeShLLuZxZIE4mp2d+ZNGYK2l5ak9hSy92o/bIav3cI6fc3OfZcDDISJcOhKeMedyzkeHygXuIR4cnHf/z2GPVYc+xDoUvOjhp15+7WHmH+rqEqUMwx0AHguepoxvm+3O65SBeyAONxvlbG04/agTS67zm2aP1YJydWTJYJRPZJlN4GsX4cA25MGua1LLxd1UPTIkLpgDw1NhYPQE0RGmFyMzNNT3FhsaRoH9fi9Ga0rBk9GKgrMVRm/1fgpLTm8reY47o8kyonek1idlNqM4F9v38gfxB1aTTeCe+cQ4AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 10\" title=\"assets 2021 08 23 10\" src=\"/static/2bcfa324cca79ac5602136b2173e884f/20982/10.png\" srcset=\"/static/2bcfa324cca79ac5602136b2173e884f/5a46d/10.png 300w,\n/static/2bcfa324cca79ac5602136b2173e884f/0a47e/10.png 600w,\n/static/2bcfa324cca79ac5602136b2173e884f/20982/10.png 778w\" sizes=\"(max-width: 778px) 100vw, 778px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    위의 결과를 볼 수 있듯이, batch norm을 적용한 model의 generator와 discriminator loss가 비정상적으로 높거나 낮은 것을 볼 수 있습니다. 이처럼 <strong>batch normalization을 사용</strong>할 때는 <code class=\"language-text\">model.train()</code>과 <code class=\"language-text\">model.eval()</code>을 <strong>반드시 사용해야한다는 것을 알 수 있습니다!</strong></p>\n<p>    <strong>이에 필연적으로</strong> <code class=\"language-text\">model.train()</code>과 <code class=\"language-text\">model.eval()</code>을 <strong>사용</strong>해야 합니다. 아래의 결과는 <code class=\"language-text\">model.train()</code>과 <code class=\"language-text\">model.eval()</code>을 사용해서 구현한 두 개의 model이 만든 MNIST입니다.</p>\n<p align=\"center\">\n    &lt; batchnorm과 initialization 적용한 model &gt;\n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 227px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 160.79295154185021%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAYAAAASYli2AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEY0lEQVRIx33W2U5VSxAG4IUos4iCDMoMKqAggwoOEAaBAAoyqQkQSYwxmpAQErjhinfwfVu+4vQOOck5F71rre7qqr/+GtYuent7U0dHR3rw4EFqbW1N7e3tqaWlJWRbW1tqbGxM9+/fTw8fPoxFxz5p9fX1pebm5liPHj1KRV1dXZqZmUn37t2L1dnZmYqiSAcHByF//foV556fPXsWcnh4uKSzt7eXqqqqYu/x48epuHv3bhi5detWoKmsrIw1MDAQl54/f54GBwfjuaGhISQ0JEcQ1tbWpvr6+kBceGCdl6mpqdTV1RXhLy0txaWhoaEwJJJXr16VkLoHgPDtuetecfv27bS8vJwqKirSzs5OOjw8DC7+/PkTnL18+TIuPXnyJL179y70VlZWwgg9XEM8OzsbDgreGa2urg6l8vLykPm9u7s7LpSVlQU1N2/evArt8gzn3jkRQSDE4fr6eoQ1NjaWFhYWQjmHDD0aIP39+3e6ceNGuri4SDU1NZEEFSKCb9++RTUUYGaSJebOnTuBhiPeOWHEJYjoNTU1ha5oJNCzUouQXRwdHY3Dp0+fhlcJmp+fj8u4wRUKcrl8+fIlLk9MTAR6VHz+/DmMFhC5AI1D5F+vufHx8VJt4pN8+/ZtOBCi6CCWsEDoRa0hlRJuOHn//n2QLQJkC/f169dxzkkuM1QAMzc3d+WY1R8/fkT6rf7+/lA6PT2NEJQRZCMjI2ljYyMq4uvXr1FSb968iSQ6Pzk5uQoZAkhyImwimre8551UzEIWAc7d8yw6wERbqCn96EVh8+SS7JL7+/ulECVNeBLFmaTk7hFBKcuZbIda7XrfSlRGw6g9tccwxChwTkYdItsl0IWGG88ZhXJQQgZAbj1ckgZI7qTt7e24F8NBifCCt56ensi2LCvY4+PjmCjCmZycDCeSkjsFKsnURTEoKEIlFEo5VPUIFfTePQfpl88aga53BqF1HiHL6ubmZikpkgCt4QnhixcvgnDn+pnB8/Pz4M88RINQFxcXr0KGyAMPJG9W7u88cJUGB/bUKh0U5ankbkwhSUE8w7L86dOnUDCl8aVvdZJu+fDhQ5ytrq5Gve7u7qa1tbUwZCoFCGUjKRDY4NV0yVK4MgmJPci0XJYciwDn0SmQUQQZWlJo8QX7Jzz8QSEZ9lQABxKGR+dkGGQEbIaglXreDQKX1ZeyoYd4hs7OzkJfyJIC5dHRURR8IFQC+KKUyyYydk3iMJcQevIkykNXUoNDBra2tsIoj9+/fy/1Jql4lREK6CmXnz9/RquhQMLytzp62Y/usMlj/gBBkYudVLQZYf50Qp3LJn8GCptmHmPQQGYIKHJKZEap9ZzRz99qbSd8pRfAWJU1pYFUbSgcY55nk9i+xExPT8fex48fwwhjSgYdBm3wnf+KCE3d8ULmvsVRhHL5/F+LPhl6ONTLuR8NU4fCy9/n+BP0PwbziuEgVEuo6knqkW6PNHCFfP1vHPnvBR3HfwEi0FVaIvU2aQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 13\" title=\"assets 2021 08 23 13\" src=\"/static/b8ce92d3f773efae47db4392cc5872f3/9b5be/13.png\" srcset=\"/static/b8ce92d3f773efae47db4392cc5872f3/9b5be/13.png 227w\" sizes=\"(max-width: 227px) 100vw, 227px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p><p align=\"center\">\n    &lt; initialization만 적용한 model &gt;\n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 243px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 160.082304526749%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAgCAYAAAASYli2AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEX0lEQVRIx4WWR09rQQyFLy2UhA4JvfdeRO81oTcpASREE4INCLFhx4oFSKz5u37vO8ERm6e3iObi8XiOz7E9BO3t7dbX12dBEFhzc7MVFRVZa2urraysWCgUspGREcvJybH6+npbW1uT3+bmptXU1FgkEpE9OzvbJiYmbH9/3wI2Zmdndaizs1MHJicn7eHhQd+Li4tWXFxs09PTdnBwIBt7+Dc1NRmAcnNzLZFI2Pr6OvuBlZSUyKGystLy8vL0ze3sFRYWCjU2vrHxd0FBgZCxspeVlaWshHBmZkYGVoL+Tu/8/Fy2ioqKTAaHh4cKenJyYpeXlwqYSqXSGeBYXl4ux+rqagvlhcRdLBaTra2tTasj5zsajWrFh/OkzBloCQgWjyfk0NjYqLWsrExiEMRtODvX7JE+mXAhWe7t7aVF6erqsrOzMx0aGhrKoBofH9c3+3A8PDxsqWRKNhQlYHd3t34AuLi4SMeBVJB4ynDjKD0tyP9NgwuC+lyGjbSlMkSvrq7KeHV1ZXV1dUpFfPy13d/fi+OBgQHtYzs9PdXFY2Njtru7KxtIt7a20mWDiqhcW1tr+fn5IhoE7FFroEMI/LBVVVVpDYfDuhzEIJybm0uLQnQcIJWgdEw8HpcNIfBpaWnJoHH0iMGFjnB7e9sCoPf39wsZqTQ0NKjeIBlHipXbOzo6bGdnRzZUZqVLQE0MvsUhgnhdsemVT9oulJMPv16HpItvT0+PugqfhYUFC7iZysfx8fFRqRH4py/t6elJgehV9rFdX1+rMm5ubiQaCBGWvg+42cn2jsGBA6RKKl5G+CIQ3GGDKi5DUChTL3OY6YJxampKB+GRPvViJyXId17pWe/t3t5epYuI6mVU9XnIJoEphcHBwYyNdXl5WTSAmjajrFCWUuEMM1KiEd1V41b+pq3gksP0MukjAj+vP4Lg6zUMXfAccOvX15duJE04ouJfX191GLJJmU7x2gQNK/0OTQS9vb1NDwdQURo4cJDbSktLM/0NGnjG7n0eCaeHLwgRAxD8oEW9zHDE4e7uTp1CPX1+fmY6BSXhVHX2UzYAgWumkRe5MiCAvxWMH1SmNlEUxOrPv3soT3p8wxWI8CUojUApaR/YBPBaIwirvx9ciA0KvG+Zh1BBx3AWOmLRmB0dHVmAws/Pz3JMJpMaDKOjo3pLuBmiOUxKjCuQfXx8KGWGxdLSkr7JSGWDMyIQ0NffXeM9DWJ+BGR8gYrO8RcTP3FM7txM2TB+OAC57+/vUpX+pCwI4g3gRc9ZfyoIprJBdoh1BzgljePjY6VKp3AJpeXl5c8p/P0eX5kBSyvhAHccIjWfyvPz86ICYdzGI4ZIpOldBpCNjQ0LKIHv72+1GaMKRCCmG0D29vamv/l/B9Q+HOCNLpOyP2BUh9wMAhx80HK7lw2ooMVfNsRgsLpg+Pn7o5QxOrEvLy9CwgHni5s91X/9PKAGLKRSe95mzo+/y7SWI/pfQJT+A3kqWSmOjgViAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 8\" title=\"assets 2021 08 23 8\" src=\"/static/989fc869768eb3039cb7f59b54e4601e/8aea6/8.png\" srcset=\"/static/989fc869768eb3039cb7f59b54e4601e/8aea6/8.png 243w\" sizes=\"(max-width: 243px) 100vw, 243px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    위의 결과를 보게 되면 두 경우에 다 결과가 잘 나온 것을 알 수 있습니다. <strong>Batchnorm을 적용한 모델</strong>은 batchnorm을 적용하지 않은 모델보다 <strong>하얀색 점들이 덜 찍혀있는 것을 확인</strong>할 수 있습니다. epoch가 경과함에 따라 generator가 내놓은 MNIST data들을 보게되면 <strong>fake image(Generator가 만든 fake MNIST data) 좀 더 선명한 것</strong>을 볼 수 있습니다. epoch에 지남에 따라 generator가 만든 fake image들은 <a href=\"\">여기</a>를 클릭하시면 볼 수 있습니다.또한 두 모델의 Discriminator Loss와 Generator Loss에도 차이가 존재합니다.</p>\n<p align=\"center\">\n    &lt; Discriminator Loss &gt; \n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 786px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 67%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABZklEQVQ4y4VTi27DIAzM/3/npm3S2oSHeRpuNmlaVvWB5JwDxnfYsBB5WLNhMwbMFegdlVn8hsIdrXWZY7Qc0GtETTT8Gj04OjGLljx63MC1YGHZ3FqDjipJzqHiyySsjgaRtyui30BEoBAGxhhBYiEXUKqgzIhlz7NoUK0VuTI+Tg7rekILwqasJaJxgWgES7CI31H+dfNue6Ii+/VkS0oZXSZ/fcbP9yeQVjlhhZ50mHy6ZGoSrMgXbIPghoe/JJGu9SLvkMhe2Y9S8JTo2f+hlneFadQukxlFHWyXgEdK3iospaDkhBzpuvBowzu8JgxRkkkXoz1Lzfo/+c/w5ZFHh4JDkObomFUehktdZ7xfvykMEd4ZWIoXlp1pNp3XqzXjS4VmO4+GzGNmnnFWOOPhj2tjnEeSW6/s96ZNu0dVcj9/+KPL+pQU9Vkp6lUK8oJyzgM12MhbV7TWDnTOXeN0v/q69gdVgP98xv+1MQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 11\" title=\"assets 2021 08 23 11\" src=\"/static/2b3fea80cb5e27cedca189da898fe770/321ea/11.png\" srcset=\"/static/2b3fea80cb5e27cedca189da898fe770/5a46d/11.png 300w,\n/static/2b3fea80cb5e27cedca189da898fe770/0a47e/11.png 600w,\n/static/2b3fea80cb5e27cedca189da898fe770/321ea/11.png 786w\" sizes=\"(max-width: 786px) 100vw, 786px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p align=\"center\">\n    &lt; Generator Loss &gt;\n</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 783px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 67.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABQ0lEQVQ4y5VTC3KFIAz0/nf1qST8wW0CxUet7bTOZFYwbjZZWJgZ7BNKrTg1zvNb6DOwSs7A8b2UDDIH1nXFwkxYySPmIkmlJc5RSt/LOV/rTlK+fNcnhNAJNw5IQjgU3kkHwYxPOd57LN45vJSw6Oa7jTqRzy0+rcc4ukIyveUk0qc27i2NlufWZ9RoCoN32G1sCjEZcDdlVvZTzqdCgmEnTsfLlP8qHO9NoXMe3rzgZDFXvM/ozzMkYkRH8CH2ijqPB4UzPh2by2VrHUp0sJaRa6/UFDSlA882joEywV9clpui5y/yjte2SesBOQbUHHGWJJEfTWmFLnPwJjTGIMbYpAcn15AO6FHSq0Rm4A46pBgfsPouYWRNR9/X0P+siFuUVUPdU2N0hiklmWmQs5kb6lrJNYdoIDUhTi5GCL5xqLgPagj/RAnUUyQAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 23 12\" title=\"assets 2021 08 23 12\" src=\"/static/78bce780a9dbbc9ca4df3c83d6a8640d/e51a6/12.png\" srcset=\"/static/78bce780a9dbbc9ca4df3c83d6a8640d/5a46d/12.png 300w,\n/static/78bce780a9dbbc9ca4df3c83d6a8640d/0a47e/12.png 600w,\n/static/78bce780a9dbbc9ca4df3c83d6a8640d/e51a6/12.png 783w\" sizes=\"(max-width: 783px) 100vw, 783px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n&nbsp; &nbsp; Discriminator는 Generator가 만든 이미지를 진짜 이미지라고 믿도록 학습되기 때문에 Discriminator Loss가 올라가야 Geneartor가 정상적으로 학습이 되고 있는 것입니다. 이것을 생각하며 위의 그래프를 보게되면 batchnorm으로 구현된 모델이 더 높은 Discriminator Loss를 갖고 있으며 차후에도 더 높아질 수 있는 가능성을 갖고 있습니다. 또한 Generator Loss를 보게 되면 batch normalization을 사용한 model이 좀더 Loss가 낮은 것을 볼 수 있습니다. 이에 batch normalization은 GAN generator의 performance를 높여준다는 결론을 얻을 수 있습니다.\n<p>   batch normalization을 적용한 모델이 구현된 코드는 아래의 사이트에 들어가시면 볼 수 있습니다😎</p>\n<p>link: <a href=\"https://github.com/gunu9441/AI/tree/main/8.GAN/Comparing_performance\">Two experiements about batchnormalized model</a></p>\n<h2 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Summary</h2>\n<ol>\n<li>batch normalization은 learnable한 parameter인 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>를 가지며 batch 단위로 들어온 input의 같은 dimension(feature)끼리 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 구하여 같은 dimension(feature)안에서 normalize합니다.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>를 통해 layer서 나온 값들을 scaling하고 shifting시킬 수 있습니다.</li>\n<li>Batch normalization은 regularization의 역할 또한 수행합니다.</li>\n<li>training 과정에서 구했던 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 활용하여 evaluation step에서 사용할 고정된 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 구하게 됩니다. training 과정에서 구했던 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>들의 평균을 통해 고정된 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">μ</span></span></span></span>와 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">σ</span></span></span></span>를 구합니다.</li>\n<li>Batch normalization layer를 추가할 때는 <code class=\"language-text\">model.train()</code>과 <code class=\"language-text\">model.eval()</code>을 꼭!!! 사용해야한다.</li>\n<li>Batch normalization을 적용하게 되면 <strong>성능이 올라간다</strong>!👍</li>\n</ol>\n<p>오늘은 batchnormalization에 대해서 알아봤습니다!:)<br>\n긴 글 읽어주셔서 감사합니다😎</p>\n<h2 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h2>\n<ul>\n<li>Batch Normalization 설명 및 사진: <a href=\"https://www.youtube.com/watch?v=HCEr5f-LfVE&#x26;list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&#x26;index=17\">https://www.youtube.com/watch?v=HCEr5f-LfVE&#x26;list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&#x26;index=17</a></li>\n<li>Batch Normalization 설명: csn231n Lecture</li>\n</ul>","frontmatter":{"title":"Batch Normalization","date":"August 22, 2021"}}},"pageContext":{"slug":"/AI/batch-normalization/","previous":{"fields":{"slug":"/AI/weight-initialization/"},"frontmatter":{"title":"Weight Initialization"}},"next":null}},"staticQueryHashes":["2486386679","3128451518"]}