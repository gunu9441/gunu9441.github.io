{"componentChunkName":"component---src-templates-blog-post-js","path":"/AI/fancier-optimization/","result":{"data":{"site":{"siteMetadata":{"title":"Gunu's AI Log","author":"[Gunu]","siteUrl":"https://gunu441.github.io","comment":{"disqusShortName":"","utterances":"gunu9441/gunu9441.github.io"},"sponsor":{"buyMeACoffeeId":"gunu9441"}}},"markdownRemark":{"id":"255907c0-15ae-502e-8577-5711cf88f70c","excerpt":"이번 시간에는 Optimization에 대해서 알아보겠습니다!✌ Deep learning에서는 forward와 back propagation을 반복하며 학습을 진행하게 됩니다. Loss function을 정의하여 forward해서 나온 결과 값과 label값을 비교하여 Loss가 얼마인지 계산하게 됩니다. 구한 Loss값을 통해 우리는 수 많은 weight에 대해 편미분 하고 빼줌으로써 각각의 weight를 update 시키죠! 이 방법은 우리가 평상시에 사용하고 있는 gradient…","html":"<p>    이번 시간에는 <strong>Optimization</strong>에 대해서 알아보겠습니다!✌ Deep learning에서는 forward와 back propagation을 반복하며 학습을 진행하게 됩니다. Loss function을 정의하여 forward해서 나온 결과 값과 label값을 비교하여 Loss가 얼마인지 계산하게 됩니다. 구한 <strong>Loss값을 통해 우리는 수 많은 weight에 대해 편미분</strong> 하고 빼줌으로써 각각의 weight를 update 시키죠! 이 방법은 우리가 평상시에 사용하고 있는 gradient descent 방법입니다. 오늘은 이 gradient descent를 base하는, 더 좋은 optimizer를 알아보도록 하겠습니다. 이번 포스팅에 대한 Category는 다음과 같습니다.</p>\n<h2 id=\"category\" style=\"position:relative;\"><a href=\"#category\" aria-label=\"category permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Category</h2>\n<ol>\n<li>SGD</li>\n<li>The Problem of SGD</li>\n<li>Momentum</li>\n<li>Adagrad</li>\n<li>RMSprop</li>\n<li>Adam</li>\n</ol>\n<h2 id=\"sgd\" style=\"position:relative;\"><a href=\"#sgd\" aria-label=\"sgd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SGD</h2>\n<p>    우리가 평상시에 사용하는 Gradient Descent는 아래와 같은 수식을 통해 parameter를 update하게 됩니다.</p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mi>t</mi></msub><mo>−</mo><mi>α</mi><mi mathvariant=\"normal\">▽</mi><mi>L</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">w_{t+1} = w_t-\\alpha \\triangledown L(w_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mord amsrm\">▽</span><span class=\"mord mathdefault\">L</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></p>\n<p>    위의 수식은 basic하고 classic한 SGD의 형태입니다. <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Loss</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span></span></span></span>를 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 대해서 편미분하고 이를 learning rate에 곱하여 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 빼주게되면 update된 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w_{t+1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span></span></span></span>이 나오게 됩니다. 하지만 <strong>SGD는 optimizor로써 다양한 문제</strong>를 갖게 되는데 어떤 문제를 갖고, 왜 발생하는지에 대해 알아보도록 하겠습니다.</p>\n<h2 id=\"the-problem-of-sgd\" style=\"position:relative;\"><a href=\"#the-problem-of-sgd\" aria-label=\"the problem of sgd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The Problem of SGD</h2>\n<p>    SGD는 아래와 같은 문제를 갖게 됩니다.</p>\n<p>① <strong>zigzag를 그리며 수렴</strong>하여 <strong>convergence speed가 느립니다</strong>.</p>\n<p>② <strong>saddle point</strong>에 빠질 수 있습니다.</p>\n<p>③ <strong>local minima</strong>에 빠질 수 있습니다.</p>\n<p>    먼저 ①에 대해서 살펴보겠습니다. 우리가 어떤 parameter <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>과 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 대해서 <strong>최적화를 진행</strong>한다면 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>z</mi></mrow><annotation encoding=\"application/x-tex\">z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span></span></span></span>축을 Loss라 했을 때 아래와 같은 그래프를 생각해볼 수 있습니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 418px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 76.33333333333334%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAACIUlEQVQ4y32UbW/TMBDH++n5FEi8QmIvgTGk8tAhTdWgo2JCiAqN0ZI0I+mcOA92EjvOn7PXtE1bsHSN7bv7+c537gA02rY9kP1h94wxvXVnt2s/OOa869A0DYlxX601DM3/Bx50E+tgDd1cN07s2kGMWYObLdgcgl2E3cL3feR5Tgs4B+vYmnYzN409yLhoO6BSffAmQqUU0jSFKAq3uYEQULk0QbAEqr5xKesuevJ7iH4PyO4ZGGOQonIKC7ROrT2ZwNJoBCrEUt0iVRwtRQoHgotalWUfGAQBwj8r5NkC2iRkRXm3dF/kFNAh01TiimSal5iKAgtVoiRdWXkQt5eoxR6woFQZ45B5iJm6xts0xydfYjQr8PJK4OxjgeHnEu+/Srz7IXHqKbwQEbz8FVoZufOxrvambSy0EjVKKJzcpXj0PMbjp3c4P/uG3+dvcD2aYvh6jpPTJZ5MFphkN3S/peVQ2tp9sQuM4xhhGNH9NE45ngs8+zLHxXKMif8BlyQjb4LhYgZPhuRsXOW1VlQcvU25A9qWscUQQoAndI8wqOn3+wq4+KUxJvnJ2of0qBhVRdlQMaqqInuOLMv6fdgNVdeuwnGcIOMpKfVaQ01fVyjIUcqSCpi7Vuu6QtWq//R2n1BN0ISAFsxYjCi6x2rFaG2oxWLwmB/49Kq8r7Rpd93POUeRF4gJZId9kinP3Is6Fszg2J/Csfk/dRaM7f5f/NiMldu8QPoAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 1\" title=\"assets 2021 08 31 1\" src=\"/static/5a71d810e29f001db4b625275851ede2/d7398/1.png\" srcset=\"/static/5a71d810e29f001db4b625275851ede2/5a46d/1.png 300w,\n/static/5a71d810e29f001db4b625275851ede2/d7398/1.png 418w\" sizes=\"(max-width: 418px) 100vw, 418px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    위의 그림에 나와있는 두 개의 parameter인 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>θ</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\theta_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>과 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>θ</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\theta_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 각각 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>과 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 mapping된다고 가정하고 설명하겠습니다. 위의 그림은 <strong>두 개의 parameter</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>과 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 대한 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Loss</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span></span></span></span>그래프 입니다. 우리는 이 두 개의 파라미터에 대해서 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Loss</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span></span></span></span>가 <strong>minimum</strong>일 때의 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>과 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 값을 알고 싶어합니다. <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">Loss</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">s</span><span class=\"mord mathdefault\">s</span></span></span></span>가 <strong>minimum이 되는 곳에 가기 위해서 gradient descent을 통해</strong> 천천히 다가가겠죠?😉</p>\n<p>    우선, <strong>초기 상태</strong>의 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>과 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>의 값이 아래 그림에 있는 <strong>하얀색 점</strong>이라고 생각해봅시다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 342px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 89.33333333333334%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADXElEQVQ4y5WU/U9bVRjH+3+44IAORnsLLEpUmDBWTDeDFpMpY/K2aBiw1THnUJkhIQHnG/EFjZOhczLNki1b4L72YpkYMEYwJMafWSkvo68qLvy09uNpoW7V1cxz873n5N6cz/0+z3PuY+GukUgkiMcTqTl5xRNxsU4r8Q/deZ7eK25Y5ufnWVhYYHFxEb/ff4ce575HGpocliRoKbDEDQEL3wxyoWeOV974CY86R9vsVaF+OmaP0T17hN4tdc+2cXTOQ8uP/Qz8Ovk3NCnL3V/6LRzDVSdTNDBOmfkueXIVFXI+rUoOp7Uc3lRz6FO20aU8iFvJJXfMytPfv54JjMfTeUgQDkbZf34Cq9HHvutuXjSL+dDYydi4A+NrCeNLCf0biTFZYshbwgE1n+YfWjKB6UUKGPsDl+9zirQqDnslRn2VKKM70QcktJMO1C4JpceB9rGNa7KdU2ounTMNWYBCodjvPOHrxKUWMCSceS+VY/Q+itIogM+UoAopzSXIrznQRx28reZxcro+GxCCsQi1k248upWxa3aUPgmvx4m+rxTt8V1oe3ah1pWieEQKzjkYVLdzaubZ7MC1WJgmn4t39Hz08yK8oyJvDRWYT4rQHxIud5eiuoXL4yJskc9+bTuemefSZ+ffwJsC2O6r4RMtH+1TO2qHhFpbglnnRKssQ3MK4KFi1F4741cljmm5tE43ZitKMuQwnZM1DAmHxjk78gkHSr0ohvthvM/vRTlgQz1RjDZiY9hrx6VYOTh9fAsYv3fILZNO+gVQvSJyeEaE3eVAfsGG+WoN+plK5LN5XNTstHuLKFIeo35mJDsw6bDu+n7atB18YQjQZTv6sAj7g0ImLuzml58H+coop90ooNwow6p20zA+lb0ooViUal8rjyg7xKZC3hIbz+oFjGiFvKc+wPDUYU5PvSScFWOb6CHvoyscevm7TeAWNPMcRmPUT31GhdnEU6I4Td9W0SHULtTsq6bWrKbabGTPxPs4L3rZe9Bg8Mh0Cnj79mY3ySjKxsYGK9EIgfAai5Fl/OGAmDfljyyJeYVAJCTexwgsR1i+ESIYWCMUChKNRkn+xpZ7taD/M1bXVjFNk0gkwurqSqbDVINNbDbW/9ZWI44nUq5u3fqT9fX11PovbtuCtsTPMWEAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 2\" title=\"assets 2021 08 31 2\" src=\"/static/4ef12dad9a9787f14905fbaa379cb4bb/e2c15/2.png\" srcset=\"/static/4ef12dad9a9787f14905fbaa379cb4bb/5a46d/2.png 300w,\n/static/4ef12dad9a9787f14905fbaa379cb4bb/e2c15/2.png 342w\" sizes=\"(max-width: 342px) 100vw, 342px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    그렇다면 우리는 저 하얀색 점에서부터 차근차근 내려가서 <strong>Loss가 minimum일 때인</strong> 빨간색 지점까지 gradient descent를 통해서 가야할 겁니다. 우리가 생각하는 <strong>ideal optimization의 방향</strong>은 아래의 그림과 같습니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 415px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 82.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAADU0lEQVQ4y52U/08TdxjH+y/tp1lBLP1GR8u+qZusruKQaZapYRrBDJJlLuo2NCRjcZtmmfLlCrS96/XaQrN1LMYYN76UkSUU2eRLYVAWS0QiYNvXPj0Qwel+8JN77p7L3b3yfj/P8znD6uoq2WxWj0L+Iiufzz/OMMTjccbHx0kMJ5iemCIzs8zdu4tM/7PM7IP7zCzNk1qaFjEl8q3xN6n793j46NEmtIA1rKyssJhZZPnhEqMDE+y3ybzV0MMhpZWqyEmqI/s4FqmgPlpBg4i6iEu/r47uxaOdZjg9pQNz+ZwONWyVfvv2Hey1MgfV73k/dpTqoIlPNAvfhm20qTbag1Z+0Gw0aXZOhMxUqk6G5ke2AwunnF6DPIPJKTxyK+6wh/rePbREXiEQsKN+Z0O5aCZwsZTgVSuy4qA57OBIyEoi3b8BzBZ8rwN19+JITP6l26xRzXwddREJ7MN3djedH+7Ce7gI75FiOutKUL6x0K45OKVaSMz/tgnMPw0cmhzhvfBePg1b8XULVU0uuj+w0fGGEe+rRUh7RNQU4z9nwic7aAhZGJ7/9fnAwckBUXAXV8J2lMsWJKHMX+1CshtptxiRnDuRDhbhP2uiSy7jjFZQ+L/AfuojTjpEAwJflCJVGfFV2ml1mWnZ+RJdrxcjCeuBS6W0CcvHhcLBrZYLY7MJFGtoop/GqBMpaMP/uUnYW7fZ5XZy2bEDyS3yul0obTaaI2VUqWWiy0P/7fKTpgiF0XJaxVgEhWXvqRI6Dhnx11i5etjM9dqXCV0RoyM6XKtZxdgcEF3+8wmQZ1g+Jiw3ixrKgTLkFjP+z0rpbNxB7Fot3uuVXBPzd0bMpUd7jf1yE4Op9DowV1DI011OiJ3xJidFbb4SMyiJOvkDDrp9Frp7Kgn/0kK9VsG7Ijw95yi/4OfWwKwOzOaepXBijAPaR7yjlnNUQE8LNY2amY/FGB1Xd3Mi7MatVvG28iWO8wqmkk5+v7kOzGW31VDf3cwuLNA3luCnZB/xZC8/J3voS0b1a3wsRmy0Vzzr58eRO4Rio8SVP1iYXiCzeE+3XFiGjV8FeV5s3bh5g5nUDOl0mrW1te0KH+/rQseeHxvv5PLiH5pjbm6OVCpFJpPRv/8X67NRWaXTy2cAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 3\" title=\"assets 2021 08 31 3\" src=\"/static/3a43b67ecf7bd5766b5a26c3a1c6eff1/73926/3.png\" srcset=\"/static/3a43b67ecf7bd5766b5a26c3a1c6eff1/5a46d/3.png 300w,\n/static/3a43b67ecf7bd5766b5a26c3a1c6eff1/73926/3.png 415w\" sizes=\"(max-width: 415px) 100vw, 415px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    하얀색 점이 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>과 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>의 초기 값이라면 당연히 gradient descent를 사용해서 위의 그림처럼 <strong>곧바로 수렴</strong>하면 좋겠죠? 하지만 <strong>gradient descent는 실제로 이렇게 동작하지 않습니다</strong>. 왜냐하면 <strong>Loss함수가 각각의 가중치들의 변화에 반응하는 정도(가중치마다 적용되는 gradient)가 다르기 때문</strong>입니다.</p>\n<p>    <strong>각각의 가중치들은 서로 다른 gradient</strong>를 갖게 됩니다. 당연히 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">w_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>~<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>n</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>(모든 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">weight</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">t</span></span></span></span>)가 서로 같은 gradient를 가질 순 없겠죠?? 그 이유는 forward pass할 때, <strong>모든 weight들이 서로 다른 input</strong>(<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span>값)을 갖게되는데 <strong>back propagation을 진행하면서 chain rule에 따라</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mrow><mi>ϑ</mi><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>w</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>ϑ</mi><mi>w</mi></mrow></mfrac><mo>=</mo><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">{\\vartheta (h(x)=wx+b)  \\over \\vartheta w} = x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.355em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.01em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">ϑ</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.485em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">ϑ</span><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">h</span><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">x</span><span class=\"mclose mtight\">)</span><span class=\"mrel mtight\">=</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault mtight\">x</span><span class=\"mbin mtight\">+</span><span class=\"mord mathdefault mtight\">b</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span>가 <strong>곱해지기 때문</strong>입니다. 또한, chain rule에 의해 앞에있는 layer들의 gradient들도 곱해지기 때문에 <strong>모든 weight들의 gradient는 다양한 값</strong>을 갖게 됩니다. 여기서 <strong>weight들 중 높은 gradient를 갖는 weight와 낮은 gradient를 갖는 weight</strong>가 생기게 되는 것입니다. 그렇다면 gradient 값이 많이 차이나는 두 개의 weight를 최적화시켜야 한다면 어떤 일이 일어날까요?</p>\n<p>    만약 Loss가 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>보다 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><strong>의 변동에 더 민감하게 반응</strong>한다면 아래와 같은 현상이 일어나게 됩니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 469px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 83.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAADTUlEQVQ4y52U/U9TVxjH+aPMNBZogRZabQsOgSgwdIRBtv2wCY443A8TSZxucUazhQWyUAot9Jbbe/uCaXR9gznYlmkgMkddCrSFzS1hupmFtP3s2MuL0+gPnuR7n3NO7v3c7znPc07JysoKW1tbPPj9AY//fcyrtEKhsNOhJBaLkUqlWLp3l9W7aaK++4SiKW7+skZsbYHIakzoOrHVaeIrWoyKcXQ1yo3UbX775+9d6BNuyZPBo4fapOL4gX37x2i8qtIgD1AvN9AsG3hXKeOMWk6fiKd9ZXQKHVMqsUmtfJtZKn6by+eL0JJdu6K51J+ovjRBV+Qi74SPccpfwbVpM+6ABXnKLFSDx29hMHSIvmANLaqJ+fX5bWBuD5jLa1D1xwXh7AJtATv94XpGgodQnEbkzyrxfqxH6tcjX63EJ1XzZchMl2JgPjv7PDBfyGvAhRkalSZ6AlU4QlamnUeZ7C3F3SH0hg53u4jdZcjXKpkQrk8XgYkXA5UFPy2yiU+kOsaGzQydbULqMDFuO4DLqsP1ugb1DhiYks18qOr5Pht/icNFibc8JpzyYZwXavii4zDDx2txVb3GuPGggAqdKEU6b8DjM9MrgC91GFic5NSkEclrQTqnZ7C5isGGWobsVsar9uOqEw47dcifV+AMWnhbLPm77MwekGeAIQHsEQ4lyYL3fBmj7eVcOWLG1VrHRL1eRB2evnJ8LhOXRfZbfEbmsnMvdhhc9PD+VAUjHjs+kU1Pj47hNw0MtVuZ6hbgMwdRvzYxHDTzXsBEg6+ZufV7e3XIc0nx0CUK+IpiwzFqY/xyNe5zRhwflTLrO8v1wEm+Uiv4IGgSpXWEo95PuZVaf6qweTYpfppFsfaFREJCFkYVK2OTdhxuCyPqSW7MjdAbtNMarOVE+CI1AxKRW2saMKcx/geU78Sxeds4rpjoUMrpVsWR84vj5tfTKeuE+0aafG3US5cw9ssc2OdkPpzaBe7uYfGciEcymyGWvE0iOSP0DTNCs8mbxZi4HyG6HBHzd4gv/Uo4ukw8+DN/pP/k4aO/BDCnOdy5dgq8WkvMJkguJ8lkMkXotsOCBi0uX9uCHRWe6mvafievXVcbGxuk02k2NzeLP/gPWEZF38xeqiQAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 4\" title=\"assets 2021 08 31 4\" src=\"/static/e71d20b34d39465f75c4332c81c6cb68/5f759/4.png\" srcset=\"/static/e71d20b34d39465f75c4332c81c6cb68/5a46d/4.png 300w,\n/static/e71d20b34d39465f75c4332c81c6cb68/5f759/4.png 469w\" sizes=\"(max-width: 469px) 100vw, 469px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    위의 그림을 좀 더 과장되게 그려보면 아래와 같이 나타나게 됩니다.</p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 899px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 21.999999999999996%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAwklEQVQY0z2P2w6CMBBE+f9vwxiNPgC+mIhKoaUtt3Idd9fAJpvpZXtmGoFqXVdM04S+76Wt1jCqgG8aWOdgSWvrYIyB4721ol3Xyfw8z8LgihgUQpCLYRjQkbqyRH6O8UkTfLIMr+SGIn/C1FagDRkwrG1bectrVgZH4zgKaNcQRviKgKcY+fUClT0IfIf+vihxS+lqAe4BGLQrh4u2bcOyLP905OTJtUpT6PcbWhtU0jVUWUEpRWf6+LL3/kjHDK4f4N4yFKncFukAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 5\" title=\"assets 2021 08 31 5\" src=\"/static/74682bbce508b8b8ff37b3d56b9adf16/681f1/5.png\" srcset=\"/static/74682bbce508b8b8ff37b3d56b9adf16/5a46d/5.png 300w,\n/static/74682bbce508b8b8ff37b3d56b9adf16/0a47e/5.png 600w,\n/static/74682bbce508b8b8ff37b3d56b9adf16/681f1/5.png 899w\" sizes=\"(max-width: 899px) 100vw, 899px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    수직방향의 weight가 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>, 수평방향의 weight가 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>이라고 해봅시다. 위의 그림을 보게 되면 Loss가 수직 방향의 가중치(<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>) 변화에 훨씬 더 민감한 것(<strong>gradient가 높은 것</strong>)을 볼 수 있습니다. 가운데 지점(smile 표정)이 Loss함수의 아래로 convex한 부분이라면 3차원의 Loss함수를 생각해봤을 때 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>변화에 대해 <strong>3차원 Loss함수가 너무 민감하게 반응(gradient가 높다)</strong>하고 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><strong>변화에는 둔감(gradient가 낮다)하기 때문에</strong> <strong>convex부분으로 가는 벡터의 크기가 현저히 달라지게 됩니다</strong>. 이에 <strong>zigzag</strong>를 그리며 optimal한 곳으로 가기 때문에 <strong>convergence speed가 너무 느린 것</strong>을 알 수 있습니다.</p>\n<p>    위에서 본 것과 같이 <strong>Loss함수는 한 방향으로는 엄청나게 민감할 수 있고 또 다른 방향으로는 덜 민감</strong>할 수 있습니다. 하지만 위에서 설명했던 것은 2개의 파라미터가 존재할 때이며 실제로는 <strong>parameter가 수천개</strong>가 될 수 있습니다. 이런 문제는 <strong>고차원 공간에서 훨씬 더 빈번하게 발생</strong>하게 됩니다. 만약 가중치가 수천개, 수천억개가 된다면 <strong>이동할 수 있는 방향이 너무나도 많고 이동할 수 있는 방향의 정도가 다양</strong>합니다. 이런 방향 중 <strong>불균형한 방향이 존재</strong>한다면 위와 같이 <strong>더 심한 zigzag</strong>를 그리며 수렴하거나 수렴하지 않을 수 있습니다(대부분 잘 동작하지 않겠죠??😊)</p>\n<p>    또한 ②, ③에서 적혀있는 것처럼 <strong>saddle point</strong>와 <strong>local minima</strong>에 빠질 수 있습니다. 아래의 그림은 <strong>saddle point</strong>와 <strong>local minima</strong>를 표현한 그림입니다.</p>\n<p>&#x3C; Local minima ></p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 361px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 52%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABH0lEQVQoz5VS22qDQBD1P0t/r5AX6+UhYiQIqeJrgzToW5QgilaDF4JGTx3DllVsaQ+MuHM5c2Z2hb7v4bouLOsNu90OmqZB13UEQQACxQnDMOAvEO73OzzPQxiGKMsSbduiKApIkoQ4jv9FNhEuHaw4iiIYhvGtkvwsRmdmzMfiAn34IF9I49d1Tdng85bg1yKsqWMJjuPA9/2Hj8uj9di2DdM0kWXZrGZ1ZEac5zkURUH5KqO/XhFcLqPqLfb7/XRpRCyK4pj3ua6QJ2Qdj8d3vDw/YbvZwDwckCTJLD9NU6iqOifkVS3/CdVoze022xlZ13XT2bIsnE4fj5H5JzEjWuyTv+1l46ZpIMvy1ED4SdVvqpdPiHA+n1FVFb4AVIcDEunZgesAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 6\" title=\"assets 2021 08 31 6\" src=\"/static/4c837ba51495ecd79f54e987fbdcc384/39d76/6.png\" srcset=\"/static/4c837ba51495ecd79f54e987fbdcc384/5a46d/6.png 300w,\n/static/4c837ba51495ecd79f54e987fbdcc384/39d76/6.png 361w\" sizes=\"(max-width: 361px) 100vw, 361px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>&#x3C; Saddle point ></p>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 269px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.39033457249071%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABJUlEQVQoz6WSS2uEMBSF82e76LLQHyO48YFYqUWZ3aADglhBZhbdtJ1aEcGVULpxcOHjtAmTTOrILDoHrl5N8uXcm5Asy0A1TZOIa0Qsy0IURexjGAYB/zeQPhRFQdM07Mc4jldBGXC/f4eu68Ilh8nvpXwRyMv0PA9hGAoodcoWUwjNJefThQqIPGAYBjab8M8E5pgCpI3EZguuybxfvu+z8ouiwHR0//Vo4Xv1dOZGhnMOmfeLqixLOI4DXdOwWq+h3d7Avr9Dst1h+3vNqqpC27bnlVDgvPn8lKnawwGfeY6Xjxy71zc8Jwnrs+u6ME0Ttm0jTVN0XSccE75Y7gMv5ZLo3LquEQQBVFVFHMenHi5Bec7hNAYax4OR1fc9uyWO84AfSKJK6Z9GoyoAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 7\" title=\"assets 2021 08 31 7\" src=\"/static/3444242943881e34e7ca8fa789715985/98fe2/7.png\" srcset=\"/static/3444242943881e34e7ca8fa789715985/98fe2/7.png 269w\" sizes=\"(max-width: 269px) 100vw, 269px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    <strong>vanilla SGD</strong>를 위의 상황에 적용한다면 <strong>학습이 진행 되지 않습니다</strong>. 왜냐하면 local minima와 saddle point에서의 <strong>gradient는 0이기 때문</strong>입니다. 이에 우리는 <strong>momentum</strong>이라는 term을 SGD에 대입시켜 위와 같은 상황을 극복할 수 있습니다.</p>\n<h2 id=\"momentum\" style=\"position:relative;\"><a href=\"#momentum\" aria-label=\"momentum permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Momentum</h2>\n<p>    Momentum을 사용한 SGD는 아래의 수식과 같습니다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>ρ</mi><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><mi mathvariant=\"normal\">▽</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">v_{t+1} = \\rho v_t + \\triangledown f(w_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">ρ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord amsrm\">▽</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mi>t</mi></msub><mo>−</mo><mi>α</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w_{t+1}=w_t-\\alpha v_{t+1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.73333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>    <strong>Momentum을 추가한 SGD</strong>는 기존의 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>를 유지합니다. <strong>vanilla SGD는 오로지 gradient 방향</strong>으로만 이동하였는데 <strong>Momentum을 추가한 SGD</strong>(이후부터는 M-SGD로 표기)는 gradient를 계산할 때 velocity를 이용하여 <strong>기존의 방향을 어느정도 유지한 채</strong>로 구하게 됩니다. 즉, 현재 미니 배치의 <strong>gradient 방향만을 고려하는 것이 아닌 이전의 velocity를 같이 고려</strong>하게 되는 것입니다(참고로 mini-batch를 사용할 경우, <strong>noise가 낀 학습데이터를 batch_size로 학습</strong>하기 때문에 살짝씩 튀면서 학습되며 수렴합니다.).</p>\n<p>    M-SGD는 하이퍼 파라미터 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">ρ</span></span></span></span>(rho)가 <strong>추가적으로 사용</strong>되고 이것은 <strong>momentum을 얼마나 반영</strong>할 것인지, 기존의 velocity를 얼마나 유지할 것인지 결정하게 됩니다. 보통 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">ρ</span></span></span></span>는 <strong>0.9나 0.99와 같은 높은 값</strong>으로 맞춰주게 됩니다. 이렇게 하여 <strong>gradient vector 그대로의 방향이 아닌 velocity vector의 방향도 고려</strong>하여 나아가게 되는 것입니다.</p>\n<p>    이 <strong>momentum term을 사용</strong>하므로써 <strong>local minima와 saddle point에서 벗어날 수</strong> 있습니다. 또한, <strong>zigzag로 수렴하는 현상도 완화</strong>시킬 수 있습니다.이전의 velocity를 계속해서 더해줌으로써 <strong>velocity가 증가</strong>하게 되어 <strong>현재의 gradient값이 작아도 velocity를 더해주기 때문에 특정 dimension(weight)에 둔감했던 loss가 빠르게 줄어들게 됩니다</strong>.</p>\n<p>    하지만 momentum을 추가한 SGD가 좁고 깊은 global minima를 지나치면 안된다고 생각할 수 있습니다. 하지만 <strong>좁고 깊은 minima는 overfitting을 유발</strong>하게 된다고 합니다. <strong>training set를 더 늘린다면 이런 좁고 깊은 민감한 minima는 사라지게</strong> 되기에 training data의 변화에 <strong>좀 더 강인한 평평한 minima</strong>를 얻길 원합니다. <strong>평평한 minima가 좀 더 일반화</strong>를 잘 할 수도 있으며 testing data에서도 더 좋은 결과를 얻을 수 있을거라고 합니다. 오히려 좁고 깊은 minima를 무시하는 것이 error가 아니라 <strong>momentum의 장점</strong>이 된다는 것입니다.</p>\n<h2 id=\"adagrad\" style=\"position:relative;\"><a href=\"#adagrad\" aria-label=\"adagrad permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AdaGrad</h2>\n<p>    SGD에 momentum term을 추가시키는 것 말고도 <strong>AdaGrad라는 최적화 방법</strong>이 존재합니다. AdaGrad는 M-SGD가 사용하는 momentum term 대신에 <strong>grad squared term을 사용</strong>하게 됩니다. AdaGrad의 구현 코드는 아래와 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">grad_squared <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n<span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n\tdx <span class=\"token operator\">=</span> compute_gradient<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\tgrad_squared <span class=\"token operator\">+=</span> dx <span class=\"token operator\">*</span> dx\n\tx <span class=\"token operator\">=</span> x <span class=\"token operator\">-</span> learning_rate <span class=\"token operator\">*</span> dx <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>grad_squared<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>    <code class=\"language-text\">grad_squared</code> term은 학습 중에 계산되는 <strong>gradient에 제곱을 하여 계속 더해</strong>주게 됩니다. weight를 update를 할 때는 앞서 계산한 <code class=\"language-text\">grad_squared</code> term으로 나눠주게 됩니다.</p>\n<p>    이 연산을 통해 <strong>gradient가 항상 높은 차원</strong>(weight)은 <code class=\"language-text\">grad_squared</code> term이 크므로 이 값을 나눠주게 되면 <strong>속도가 떨어지게 되고</strong>, <strong>gradient가 항상 낮은 차원</strong>(another weight)은 <code class=\"language-text\">grad_squared</code> term이 작아 이 값을 나눠주게 되면 <strong>속도가 붙게 됩니다</strong>.</p>\n<p>    하지만 학습 횟수가 늘어나게 되면 <code class=\"language-text\">grad_squared</code> term이 커지게 되는데 이 값을 나눠주게 되면 <strong>step size가 서서히 작아지게</strong> 됩니다. 이 현상은 <strong>loss함수가 convex</strong>한 경우, <strong>minmum에 근접하게 될 때 서서히 속도를 줄여서 수렴할 수 있기 때문에 좋은 특징</strong>이 됩니다. 하지만 <strong>non-convex 한 경우</strong>에서는 문제가 될 수 있습니다. <strong>saddle point에 걸려 AdaGrad가 멈춰버릴 수 있기 때문</strong>입니다. 이에 AdaGrad의 변형인 <strong>RMSProp</strong>이 등장하게 됩니다.</p>\n<h2 id=\"rmsprop\" style=\"position:relative;\"><a href=\"#rmsprop\" aria-label=\"rmsprop permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RMSProp</h2>\n<p>    RMSProp에서는 AdaGrad의 <code class=\"language-text\">grad_squared</code>를 그대로 사용하게 됩니다. 하지만 조금 변형된 <code class=\"language-text\">grad_squared</code> term을 사용하는데 RMSProp의 코드는 아래와 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">grad_squared <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n<span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n\tdx <span class=\"token operator\">=</span> compute_gradient<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\tgrad_squared <span class=\"token operator\">=</span> decay_rate <span class=\"token operator\">*</span> grad_squared <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">-</span>decay_rate<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> dx <span class=\"token operator\">*</span> dx\n\tx <span class=\"token operator\">=</span> x <span class=\"token operator\">-</span> learning_rate <span class=\"token operator\">*</span> dx <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>grad_squared<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>    위의 코드를 보게 되면 기존의 AdaGrad의 <code class=\"language-text\">grad_squared</code>와 다르다는 것을 볼 수 있습니다. <strong>기존의 AdaGrad는 무작정 현재의 gradient를 제곱</strong>해서 더했지만 <strong>RMSProp는</strong> <code class=\"language-text\">decay_rate</code><strong>을 추가</strong>시켜 <strong>지수 가중 이동 평균(Exponential weighted moving average)을 사용</strong>하므로써 <strong>얼마나 이전의 grad_squared 값을 반영</strong>할 건지 결정할 수 있습니다. 또한, <strong>현재 gradient의 제곱값을 (1-decay_rate)만큼 곱</strong>해주어 반영하게 됩니다.</p>\n<p>    RMSProp에서는 <strong>decay_rate를 보통 0.9나 0.99</strong>를 사용하게 되며 현재 <strong>gradient의 제곱을 계속해서 더하고 나눠준다는 점이 AdaGrad와 유사</strong>합니다. 이를 통해 두 개의 optimizer는 <strong>step의 속도를 감속</strong>시키거나 <strong>가속</strong>시킬 수 있습니다.</p>\n<p>    하지만 <strong>RMSProp</strong>은 step size가 점점 줄어드는, 즉 점점 <strong>속도</strong>가 줄어드는 문제를 해결하였습니다.</p>\n<p>    지금까지 SGD와 M-SGD, AdaGrad, RMSProp에 대해서 알아보았습니다. Momentum은 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>(momentum)를 <strong>이용해서 step을 조절</strong>하고 AdaGrad와 RMSProp은 <code class=\"language-text\">grad squared</code> term을 나눠주는 방식으로 <strong>step을 조절</strong>해주었습니다. 만약 Momentum에서 사용한 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>의 개념과 RMSProp에서 사용했던 <code class=\"language-text\">grad squared</code> term을 조합하면 어떨까요?😉</p>\n<h2 id=\"adam\" style=\"position:relative;\"><a href=\"#adam\" aria-label=\"adam permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adam</h2>\n<p>    Momentum과 AdaGrad, RMSprop의 개념을 조합하여 만든 optimization algorithm이 바로 Adam입니다. Adam은 <code class=\"language-text\">first_moment</code>와 <code class=\"language-text\">second_moment</code>를 이용해서 <strong>이전의 정보를 유지</strong>시키는데 Adam의 <code class=\"language-text\">first_moment</code>는 Momentum에, <code class=\"language-text\">second_moment</code>는 AdaGrad와 RMSProp에 대응되는 개념입니다. Adam의 코드는 아래와 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">first_moment <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nsecond_moment <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n<span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n\tdx <span class=\"token operator\">=</span> compute_gradient<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\tfirst_moment <span class=\"token operator\">=</span> beta1 <span class=\"token operator\">*</span> first_moment <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">-</span>beta1<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> dx\n  second_moment <span class=\"token operator\">=</span> beta2 <span class=\"token operator\">*</span> second_moment <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">-</span>beta2<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> dx <span class=\"token operator\">*</span> dx\n\tx <span class=\"token operator\">=</span> x <span class=\"token operator\">-</span> learning_rate <span class=\"token operator\">*</span> first_moment <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>second_moment<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>    <code class=\"language-text\">first_moment</code>부터 살펴보게 되면 <strong>Momentum때와는 다르게 가중합을 사용</strong>하였습니다. Momentum의 식은 아래와 같습니다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>ρ</mi><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><mi mathvariant=\"normal\">▽</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">v_{t+1} = \\rho v_t + \\triangledown f(w_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">ρ</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord amsrm\">▽</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p>    위의 수식을 보게되면 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>는 exponential weighted moving average를 사용하지 않았는데 <code class=\"language-text\">first_moment</code>에서는 <strong>gradient의 exponential weighted moving average를 사용</strong>한 것을 볼 수 있습니다. 하지만 <code class=\"language-text\">first_moment</code>는 Momentum에서의 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>와 의미적으로 같습니다.</p>\n<p>    <code class=\"language-text\">second_moment</code>는 AdaGrad와 RMSProp처럼 <strong>gradients의 제곱을 이용</strong>합니다. AdaGrad와 RMSProp와 같이 <strong>sqrt(second_moment)를 나눠주게 되며 똑같은 역할</strong>을 하게 됩니다(<strong>큰 값을 갖게 되면 작은 step</strong>으로, <strong>작은 값을 갖게 되면 큰 step</strong>으로 학습 진행).</p>\n<p>    이에 Adam의 형태를 <strong>momentum + grad squared term으로 구성</strong>되어 있는 것으로 볼 수 있습니다. 하지만 현재의 Adam은 문제가 있습니다.</p>\n<p>    초기에 <code class=\"language-text\">second_moment</code><strong>를 0으로 초기화</strong>하여 epoch를 돌때마다 <code class=\"language-text\">second_moment</code>는 update를 거치게 됩니다. 하지만 <strong>beta2</strong>는 <code class=\"language-text\">decay_rate</code>로 0.9나 0.99의 값을 가지기 때문에 <code class=\"language-text\">second_moment</code><strong>는 0에 가까운 값을 초기 상태</strong>에서 갖게됩니다. weight를 update 해줄 때 <code class=\"language-text\">second_moment</code>로 나누기 때문에 <strong>초기의 step이 엄청나게 커지게</strong> 됩니다. 초기의 step이 엄청나게 커진 것은 손실 함수자체가 가파르기 때문이 아니라 <code class=\"language-text\">second moment</code>를 0으로 초기화 시켜 발생한 <strong>인공적인 현상이기에 문제</strong>가됩니다.</p>\n<p>    <code class=\"language-text\">first_moment</code> 또한 작아서 상쇄될거라 생각할 수 도있지만 <strong>상쇄되지 않고 초반 step이 엄청나게 커져 엉뚱한 곳으로 이동</strong>하여 잘못될 수 도 있습니다. 이에 문제를 해결하고자 <strong>보정항</strong>(bias correction term)을 따로 추가하게 됩니다.</p>\n<p>    bias correction term을 추가한 Adam의 코드는 아래와 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">first_moment <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\nsecond_moment <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n<span class=\"token keyword\">for</span> t <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_iterations<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\tdx <span class=\"token operator\">=</span> compute_gradient<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\t<span class=\"token comment\"># Momentum</span>\n\tfirst_moment <span class=\"token operator\">=</span> beta1 <span class=\"token operator\">*</span> first_moment <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">-</span>beta1<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> dx\n\n\t<span class=\"token comment\"># AdaGrad, RMSProp</span>\n    second_moment <span class=\"token operator\">=</span> beta2 <span class=\"token operator\">*</span> second_moment <span class=\"token operator\">+</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">-</span>beta2<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> dx <span class=\"token operator\">*</span> dx\n\n\t<span class=\"token comment\"># Bias correction</span>\n\tfirst_unbias <span class=\"token operator\">=</span> first_moment <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> beta1 <span class=\"token operator\">**</span> t<span class=\"token punctuation\">)</span>\n\tsecond_unbias <span class=\"token operator\">=</span> second_moment <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> beta2 <span class=\"token operator\">**</span> t<span class=\"token punctuation\">)</span>\n\n\t<span class=\"token comment\"># AdaGrad, RMSProp</span>\n\tx <span class=\"token operator\">=</span> x <span class=\"token operator\">-</span> learning_rate <span class=\"token operator\">*</span> first_moment <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>second_moment<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>    <code class=\"language-text\">first_moments</code>와 <code class=\"language-text\">second_moments</code>를 update하고 난 후 현재 step에 적절한 bias correction term(first<em>unbias와 second</em>unbias)을 계산합니다. 이 bias correction term을 통해 초기 step에서 기존 Adam의 작은 <code class=\"language-text\">second_moment</code>로 나눠줄 때 <strong>step이 엄청 커지는 문제를 해결</strong>할 수 있습니다. <code class=\"language-text\">second_unbias</code>에 있는 <code class=\"language-text\">beta2 ** t</code>는 <strong>초기 step에서 값이 0.9나 0.99</strong>가 됩니다. 이 값을 1에 빼게 된다면 <strong>0.1이나 0.01이 나오게 되는데</strong> 이 값으로 <code class=\"language-text\">second_moment</code>를 나눠줌으로써 <code class=\"language-text\">second_moment</code>의 값이 <strong>10배, 100배로 늘어나며 기존의 문제를 해결</strong>할 수 있습니다. 초기 step에서 기존 Adam의 <code class=\"language-text\">second_moment</code>값이 작은게 문제였기 때문에 <strong>이 값을 10배, 100배로 늘려줘서 보정을 한 것</strong>입니다. <code class=\"language-text\">first_moment</code> 또한 <strong>기존 Adam에서의 초기 step에서 매우 작은 값</strong>을 가지고 있었지만 <strong>늘려주게 되면서 보정</strong>됩니다.</p>\n<p>    Adam은 <strong>다양한 문제들에도 잘 작동</strong>하여 모든 문제에서 <strong>기본 알고리즘으로 Adam을 사용</strong>한다고 합니다. 실제로 Adam은 <code class=\"language-text\">beta1</code>=0.9, <code class=\"language-text\">beta2</code>=0.999, <code class=\"language-text\">learning_rate</code>=1e-3 or 5e-4(1e-4~1e-3)가 <strong>많은 모델의 첫 시작 포인트</strong>로 좋습니다.</p>\n<h2 id=\"compare\" style=\"position:relative;\"><a href=\"#compare\" aria-label=\"compare permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Compare</h2>\n<p align=\"center\">\n   <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 484px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 50.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACY0lEQVQoz3WS3UuTYRiH9z8EnaSklkbqdMswQsoOPMgDg7SOtA+tPGiKmngyS0qF8CTMgiDK/AhFS9O5+a6oiNRlzrlZ0YIoczqde4fMOW3NbVevrykJ9YObm+fgd3HB/ShePvjKlevjlA2NoJm4xdXxUtomSnht0TD6ToN5pIRhUyH2H+2sJxwOSRMmEolszXo2t6KqzkxUQwd7jWfJ1MdTMxhDZ1cM/bfj0NXGMVCfQE/TDsYnqvhf/oYrKk0mCXaO3OeJ3DMmYWxRYtSqGLyoZrBQjVB6kP76WGzmarksigvYbBbc7gWcTieBQGC7Ybm1UTJLkGCJCPeTMGhSMZyUYMclaI40RQd4Vrcbq0UrF5Z9KzimPSwt+fF4PDJwm+ENSwU1hhjZzFCSgjFHiXBEAmVI0OxUBovV9N6JZvjDH8PFMJN2cM77cc07cTgczMzMMDU1JdmLKDps5XR0S0CtZJarpis9k77kdISj0jtPhbFGRWt3FE8+bgDDix748plV0Y1jdpa5uTl8Ph9ra2uEQiEUb6xl9DfFIhSrGMhKpzXlGLp1uxMqDGWSbbuSKkM0DZPXZGBo1UdEdPJreQm36MHlcuH1ercOpBgbq0BXH49wIQ0hO43ujEP0nU7DWJ6C0Kzk7otkDuv3Uf3poVzwiSFmrRG8YoCp6e8sL/u3XVrxfkRDb+0u9JcT0Z/fR09eEo80avrb47hp2EOWsJ+dvUVo35rlYjAQ4qc3JG//ip9gMLgdaLc3MtSTh6n1DMMt+Zg689E9vkSbqYDK0XwKzHWcevqK5p5vG0Ui//yHm/kNCGNwfKvClXgAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"assets 2021 08 31 8\" title=\"assets 2021 08 31 8\" src=\"/static/6c7738aea9155718cfc6752630613d33/ff42b/8.png\" srcset=\"/static/6c7738aea9155718cfc6752630613d33/5a46d/8.png 300w,\n/static/6c7738aea9155718cfc6752630613d33/ff42b/8.png 484w\" sizes=\"(max-width: 484px) 100vw, 484px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n</p>\n<p>    위의 그림에서 검정색은 SGD, 파란색은 M-SGD, 빨간색은 RMSProp, 보라색은 Adam을 사용했을 때의 결과입니다. Momentum은 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>를 이용해서 <strong>step을 조절</strong>하고 AdaGrad와 RMSProp은 <strong>grad squared term을 나눠주는 방식</strong>으로 step을 조절해줍니다. 이에 momentum의 경우, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 덕분에 <strong>overshoot한 이후에 다시 minima로</strong> 돌아오게 됩니다. 하지만 RMSProp의 경우 gradient가 크면 작은 step으로 줄어들고, gradient가 작다면 좀 더 큰 step으로 줄어들기 때문에 <strong>각 차원마다(각 weight마다) 상황에 맞도록 수렴</strong>합니다.</p>\n<p>    Adam은 <strong>momentum</strong>과 <strong>RMSProp</strong>을 섞은 듯한 모습을 보여줍니다. Adam이 momentum처럼 <strong>overshoot을 하긴 하지만 momentum 만큼 심하게 overshooting되지 않게 됩니다</strong>. 왜냐하면 현재 gradient에 <code class=\"language-text\">(1-beta1)</code>을 곱해서 더해주며(가중합을 말하는겁니다😉) <strong>현재 gradient가 높으면 작은 step</strong>으로 가게끔 보정해주고 <strong>낮으면 큰 step으로 가게끔 보정</strong>(grad<em>squared &#x26; unbiased term)해주기 때문입니다. 이에. Adam은 RMSProp의 특징을 갖고있다고 말할 수 있습니다. 위에서 말했던 것처럼 **overshooting의 정도를 줄여주는데 grad</em>squared term과 unbiased term이 기여하기 때문<strong>입니다. 이에 **Adam또한 각 차원(수많은 weight들)의 상황을 고려</strong>해서 <strong>step을 이동</strong>한다고 말할 수 있습니다.</p>\n<p>    따라서 Adam은 <strong>momentum처럼 동작하는 동시에 RMSProp처럼 동작</strong>합니다.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>    많은 optimization algorithm을 알아보았지만 Adam을 가장 많이 사용한다고 합니다. 지금까지 optimization algorithm들이 <strong>training error를 줄이고 손실함수를 최소화시키기 위해 동작하는 것처럼 설명</strong>했지만 사실 <strong>우리의 목표는 training error를 줄이는 것이 아닙니다</strong>. 우리의 목표는 한번도 보지 못한 데이터, 즉 <strong>test(validation) dataset에 대한 성능을 끌어올리는 것이 목표</strong>입니다. 이에 training error를 줄이는 것도 좋지만 우리가 정말 하고자 하는 것은 <strong>test dataset에 대한 error를 줄이는 것</strong>입니다. 이에 <strong>모델이 학습될 수록 떨어지는 training error</strong>와 <strong>한번도 보지못한 데이터인 test datset의 error의 격차를 줄이는 것이 중요</strong>합니다. 그렇다면 우리가 Loss에 대해 최적화를 모두 마친 상태에서 한번도 보지 못한 데이터인 test dataset에서의 성능을 올리기 위해서는 어떻게 해야할까요?</p>\n<p>    첫번째 방법은 <strong>Ensemble</strong> 입니다. <strong>여러개의 모델을 독립적으로 학습</strong>시켜 <strong>모델들의 결과의 평균</strong>을 이용하는 방법입니다. <strong>모델이 늘어날 수록 overfitting이 줄어들게 되며 성능이 향상</strong>되게 되는데 보통 2%정도 상승하게 된다고 합니다. 그렇다면 단일모델에서의 validation set에 대한 성능을 올리려면 어떻게 해야할까요?</p>\n<p>이 방법은 다음 포스팅 때 올리도록 하겠습니다!<br>\n감사합니다😎</p>\n<h2 id=\"summary\" style=\"position:relative;\"><a href=\"#summary\" aria-label=\"summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Summary</h2>\n<ol>\n<li>SGD는 <strong>Loss값이 각 parameter(<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">weight</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">h</span><span class=\"mord mathdefault\">t</span></span></span></span>)의 변동에 따라 민감한 정도가 다르기 때문</strong>에 <strong>zigzag</strong>를 그리면서 수렴하므로 convergence speed가 느려질 수 있으며, <strong>saddle point와 local minima</strong>에 빠질 수 있습니다. <strong>고차원 공간(여러개의 weight)에서</strong>는 <strong>zigzag로 수렴</strong>하는 현상, <strong>saddle point, local minima</strong>에 더 잘 빠질 수 있습니다.</li>\n<li>SGD에 <strong>momentum term</strong>을 추가시키면 기존의 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>를 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">ρ</span></span></span></span>만큼 유지한채로 현재 gradient값을 더하여 update하기 때문에 <strong>saddle point</strong>와 <strong>local minima</strong>에서 빠져나올 수 있습니다.</li>\n<li>M-SGD의 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">ρ</span></span></span></span>값을 보통 <strong>0.9</strong>나 <strong>0.99</strong>로 설정합니다.</li>\n<li><strong>좁고 깊은 minima(=overfitting)를</strong> 원하는 것이 아니라 <strong>평평한 minima</strong>를 찾는 것을 optimizer는 목표로 합니다.</li>\n<li>AdaGrad는 Neural Network를 학습시킬 때 잘 사용하지 않습니다.</li>\n<li>RMSProp에서는 <strong>decay_rate</strong>를 보통 <strong>0.9</strong>나 <strong>0.99</strong>를 사용합니다.</li>\n<li>Momentum은 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>c</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">velocity</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">v</span><span class=\"mord mathdefault\">e</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathdefault\">o</span><span class=\"mord mathdefault\">c</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">t</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>(momentum)를 이용해서 <strong>step을 조절</strong>하고 AdaGrad와 RMSProp은 <strong>grad squared term</strong>을 나눠주는 방식으로 <strong>step을 조절</strong>한다.</li>\n<li>초기 <code class=\"language-text\">seoncd_moment</code>가 너무 작아 <strong>step이 너무 커지기 때문에 발산</strong>할 수 있습니다. 이에 Adam에서는 <strong>보정항</strong>(bias correction term)을 사용합니다.</li>\n<li>Adam은 <code class=\"language-text\">beta1</code>=0.9, <code class=\"language-text\">beta2</code>=0.999, learning_rate=1e-3 or 5e-4(1e-4~1e-3)가 많은 모델의 첫 시작 포인트로 좋습니다.</li>\n<li>Optimizer를 사용할 때 Adam을 사용합시다!😎</li>\n</ol>\n<h1 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h1>\n<ul>\n<li>SGD+Momentum: <a href=\"https://hyunw.kim/blog/2017/11/01/Optimization.html\">https://hyunw.kim/blog/2017/11/01/Optimization.html</a></li>\n<li>Difference between SGD and GD: <a href=\"https://datascience.stackexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent\">https://datascience.stackexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent</a></li>\n<li>GD vs BGD vs MSGD: <a href=\"https://skyil.tistory.com/68\">https://skyil.tistory.com/68</a></li>\n<li>cs231n Lecture</li>\n</ul>","frontmatter":{"title":"Fancier Optimization","date":"August 31, 2021"}}},"pageContext":{"slug":"/AI/fancier-optimization/","previous":{"fields":{"slug":"/AI/affine-transformation/"},"frontmatter":{"title":"[Data Augmentation]Affine Transformation"}},"next":null}},"staticQueryHashes":["2486386679","3128451518"]}