---
title: Weight Initialization
date: 2021-08-21 12:08:77
category: AI
thumbnail: { thumbnailSrc }
draft: false
---

&nbsp; &nbsp; ì´ë²ˆ ì‹œê°„ì—ëŠ” weight initializationì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤!ğŸ˜Š activation functionì„ sigmoidë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ì–‘ìª½ì˜ saturation area ë•Œë¬¸ì— gradient vanishing í˜„ìƒì´ ìƒê²¨ í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šì„ ìˆ˜ê°€ ìˆì–´ì„œ **ReLUë‚˜ Leaky ReLUë¥¼ ì‚¬ìš©**í•˜ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ activation functionì„ ì˜ ì„¤ì •í•´ë„ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ë§¤ë²ˆ í•™ìŠµë˜ëŠ” ì†ë„(costê°€ ë–¨ì–´ì§€ëŠ” ì†ë„)ê°€ ë‹¬ë¼ì§€ê±°ë‚˜ í•™ìŠµì´ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ”ë° ì´ëŠ” weight initializationì˜ ì˜í–¥ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\1.png"/>
</p>

&nbsp; &nbsp; ì´ë²ˆ í¬ìŠ¤íŠ¸ì˜ ì¹´í…Œê³ ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

## Category

1. The Basic Concept of Weight Intilization
2. Weight Initialization Problems
3. Weight Initialization Methods
4. More Details about Xavier Initialization & He Initialization
5. Code
6. Summary
7. Reference

## The Basic Concept of Weight Initialization

<p align="center">
   <img src="assets\2021-08-21\1.png"/>
</p>

&nbsp; &nbsp; ìš°ë¦¬ê°€ ëª¨ë¸ì„ ì•Œë§ê²Œ ì„¤ì •í•˜ê³  activation functionì„ ì˜ ì„ íƒí•´ë„ ìœ„ì— ë´¤ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ë§¤ë²ˆ í•™ìŠµë˜ëŠ” ì†ë„(costê°€ ë–¨ì–´ì§€ëŠ” ì†ë„)ê°€ ë‹¬ë¼ì§€ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” weight initializationì„ í•´ì£¼ì§€ ì•Šê³  weight ê°’ì„ randomí•˜ê²Œ ì£¼ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì–´ë–»ê²Œ weight ê°’ì„ ì´ˆê¸°í™”í•´ì£¼ë©° ì¢…ë¥˜ì—ëŠ” ë¬´ì—‡ì´ ìˆëŠ”ì§€ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

## Weight Initialization Problems

weightë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì´ˆê¸°í™” í•´ì£¼ê²Œ ë˜ë©´ ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.

**â‘  ì‘ê²Œ ì´ˆê¸°í™”í•˜ëŠ” ê²½ìš°**

1. **0.000..** ì´ë˜ë„ë¡ **weightë¥¼ ë„ˆë¬´ 0ì— ê°€ê¹ê²Œ ì´ˆê¸°í™”** í•˜ëŠ” ê²½ìš°
2. **ë§ì€ layerë¥¼ ì‚¬ìš©**í•˜ë©´ì„œ **0ì— ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ weightë¥¼ ì´ˆê¸°í™”** í•˜ëŠ” ê²½ìš°

**â‘¡ í¬ê²Œ ì´ˆê¸°í™”í•˜ëŠ” ê²½ìš°**

ìœ„ì™€ ê°™ì€ ê²½ìš°ì— ì™œ weight initialization problemì´ ìƒê¸°ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

---

**â‘  ì‘ê²Œ ì´ˆê¸°í™”í•˜ëŠ” ê²½ìš°**

&nbsp; &nbsp; ì‘ê²Œ ì´ˆê¸°í™” í•˜ëŠ” ê²½ìš°, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ codeë¡œ weightë¥¼ ì´ˆê¸°í™”í–ˆë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# Standard Gaussian Distribution which has standard deviation = 0.01
W = np.random.randn(fan_in,fan_out) * 0.01
```

&nbsp; &nbsp; weightê°€ **0.000..** ì´ë˜ë„ë¡ **weightë¥¼ ë„ˆë¬´ 0ì— ê°€ê¹ê²Œ ì´ˆê¸°í™”** í•˜ëŠ” ê²½ìš° í•™ìŠµì´ ì „í˜€ ì§„í–‰ë˜ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤. ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\2.png"/>
</p>

&nbsp; &nbsp; ${\vartheta g \over \vartheta x} =w= 0.00..$ì´ê¸° ë•Œë¬¸ì— ${\vartheta f \over \vartheta x} = {\vartheta f \over \vartheta g} * {\vartheta g \over \vartheta x} = 0*1 = 0$ì´ ë©ë‹ˆë‹¤. ì´ì— **ì € $x$ì™€ ì—°ê²°ë˜ì–´ ìˆëŠ” ëª¨ë“  ì´ì „ nodeë“¤ì˜ gradientëŠ” 0ì´ ë˜ë¯€ë¡œ gradient vanishing í˜„ìƒ**ì´ ìƒê¸°ê²Œ ë©ë‹ˆë‹¤. ì´ì— í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ” ìƒí™©ì´ ìƒê¸°ê²Œ ë˜ë¯€ë¡œ weightë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

&nbsp; &nbsp; ì´ì œ, **ë§ì€ layerë¥¼ ì‚¬ìš©**í•˜ë©´ì„œ **0ì— ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ weightë¥¼ ì´ˆê¸°í™”** í•˜ëŠ” ê²½ìš°ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

&nbsp; &nbsp; activation functionìœ¼ë¡œ tanhë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ê²½ìš°, ë ˆì´ì–´ë¥¼ í†µê³¼í•˜ë©´ í†µê³¼í•  ìˆ˜ë¡ ëª¨ë“  activation mapì˜ ê°’ë“¤ì´ 0ì— ê°€ê¹Œì›Œì§€ê²Œ ë©ë‹ˆë‹¤. 0ì— ê°€ê¹Œìš´ weightë“¤ì´ ê³„ì† ê³±í•´ì§€ë©´ì„œ activationë“¤ì´ 0ì— ê°€ê¹Œì›Œì§€ê²Œ ë˜ê³  tanhë¥¼ í†µê³¼í•˜ë©´ì„œ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ ë©ë‹ˆë‹¤. ë‹¤ì‹œ ì´ ê°’ë“¤ì´ 0ì— ê°€ê¹Œìš´ weightì™€ ê³±í•´ì§€ê²Œ ë˜ë©´ì„œ ë ˆì´ì–´ê°€ ê¹Šì–´ì§ˆ ìˆ˜ë¡ ëª¨ë“  ê°’ë“¤ì´ 0ìœ¼ë¡œ ìˆ˜ë ´ë˜ê²Œ ë©ë‹ˆë‹¤.

&nbsp; &nbsp;ìœ„ì™€ ê°™ì€ í˜„ìƒì´ ì§„í–‰ëœë‹¤ë©´ back propagation stepì—ì„œ gradient vanishingí˜„ìƒì´ ì¼ì–´ë‚˜ê²Œ ë©ë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\3.png"/>
</p>

forward passë¥¼ í–ˆì„ ë•Œ, ë“¤ì–´ì˜¨ $x$ê°’ì´ 0ì´ ëœë‹¤ë©´ backpropagationì„ ì§„í–‰ í•  ë•Œ, upstream gradientì™€ local gradientë¥¼ ê³±í•  ê²½ìš° 0ì´ ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê°™ìŠµë‹ˆë‹¤

$$
{\vartheta f \over \vartheta w} = {\vartheta f \over \vartheta g} * {\vartheta g \over \vartheta w} = 1*0 = 0
$$

&nbsp; &nbsp; ìœ„ì˜ ìˆ˜ì‹ì—ì„œ ${\vartheta f \over \vartheta g}$ ì€ upstream gradientì´ë©° local gradientëŠ” ${\vartheta g \over \vartheta w}$ ì…ë‹ˆë‹¤. ì´ê²ƒì€ í•˜ë‚˜ì˜ layerë§Œ ë³´ê³  ì„¤ëª…í•œ ê²ƒì¸ë° networkë¥¼ ì „ì²´ë¡œ ì„¤ëª…í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

&nbsp; &nbsp; ìš°ë¦¬ëŠ” í•™ìŠµì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ${\vartheta loss \over \vartheta w}$ ë¥¼ êµ¬í•´ì•¼í•©ë‹ˆë‹¤. ì´ ê°’ì€ **chain rule**ì„ í†µí•´ êµ¬í•´ ë‚˜ê°€ì•¼í•˜ë©° ì•„ë˜ì˜ ì—°ì‚°ì´ ì§„í–‰ë©ë‹ˆë‹¤.

$$
{\vartheta loss \over \vartheta w} = {\vartheta loss \over \vartheta f} * {\vartheta f \over \vartheta g} * {\vartheta g \over \vartheta w}
$$

&nbsp; &nbsp; ìœ„ì˜ ì—°ì‚°ì„ í•˜ê¸° ìœ„í•´ì„œ upstream gradientì™€ local gradient(=$x$)ë¥¼ ê³±í•´ì•¼í•˜ëŠ”ë° local gradientì¸ ${\vartheta g \over \vartheta w}$ê°€ 0ì´ ë˜ê¸° ë•Œë¬¸ì— updateê°€ ì¼ì–´ë‚˜ì§€ ì•Šê²Œ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆë“¯ì´, **activationì˜ ê°’ì˜ ë¶„í¬ë¥¼ ë³´ê³  í•™ìŠµì´ ì˜ë  ìˆ˜ ìˆëŠ”ì§€ ë  ìˆ˜ ì—†ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤**. **activation ê°’ ì¤‘ 0ì´ ë§ìœ¼ë©´ local gradientê°€ 0ì´ë˜ì–´ í•™ìŠµì— ì•…ì˜í–¥**ì„ ë¯¸ì¹˜ë‹ˆê¹Œìš”ğŸ˜‰

â‘¡ í¬ê²Œ ì´ˆê¸°í™”í•˜ëŠ” ê²½ìš°

&nbsp; &nbsp; ë‹¤ìŒì€ í¬ê²Œ ì´ˆê¸°í™”í•˜ëŠ” ê²½ìš°ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì‘ê²Œ ì´ˆê¸°í™”í•œ ê²½ìš°, ê°€ìš°ì‹œì•ˆ í‘œì¤€ì •ê·œë¶„í¬ì˜ í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë‘ì—ˆì§€ë§Œ ì§€ê¸ˆì€ 1ë¡œ ì„ ì–¸í•©ë‹ˆë‹¤.

```python
# Standard Gaussian Distribution which has standard deviation = 1.0
W = np.random.randn(fan_in,fan_out) * 1.0
```

&nbsp; &nbsp; ì´ ë•Œ, **tanh** activation functionì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ layerë¥¼ í†µê³¼í–ˆì„ ë•Œ, activationì´ -1ê³¼ 1ë¡œ ì¹˜ì¤‘ë˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

&nbsp; &nbsp; í‘œì¤€ í¸ì°¨ê°€ í° ê°’ìœ¼ë¡œ ë‚œìˆ˜ë¥¼ ìƒì„±í•˜ì—¬ weightë¥¼ ì´ˆê¸°í™”í•´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´í›„ $x$ì™€ í–‰ë ¬ê³±ì„ ì§„í–‰í•˜ê²Œ ë˜ë©´ ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ê±°ë‚˜ í° ê°’ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. ì´ ê°’ë“¤ì´ tanhë¥¼ ì§€ë‚˜ê²Œ ëœë‹¤ë©´ $x$ì¶•ì˜ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ì— ìœ„ì¹˜í•´ì„œ tanhì™€ mappingë˜ê¸° ë•Œë¬¸ì— -1 ì´ë‚˜ 1ê°’ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì€ $y = tanh(x)$ì˜ ê·¸ë˜í”„ ì…ë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\4.png"/>
</p>

## Weight Initialization Methods

&nbsp; &nbsp; weightë¥¼ ì´ˆê¸°í™” í•˜ëŠ” 2ê°€ì§€ ë°©ë²•ì„ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. í•˜ë‚˜ëŠ” Xavier initializationì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” He initialization ì…ë‹ˆë‹¤. ë¨¼ì € Xavier initialization ë¶€í„° ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

â‘  Xavier initialization

&nbsp; &nbsp; Xavier Initialization ì€ uniform distribution í˜¹ì€ normal distributionìœ¼ë¡œ weightë¥¼ ì´ˆê¸°í™”í•  ë•Œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê²Œ ë˜ë©° uniform distributionì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë²”ìœ„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\5.png"/>
</p>

&nbsp; &nbsp; í•˜ì§€ë§Œ, ì •ê·œ ë¶„í¬ë¡œ ì´ˆê¸°í™”í•  ê²½ìš° í‰ê· ì´ 0ì´ê³  í‘œì¤€ í¸ì°¨ê°€ ì•„ë˜ì™€ ê°™ì€ $\sigma$ë¥¼ ê°€ì§€ë„ë¡ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\6.png"/>
</p>

&nbsp; &nbsp; ë”°ë¼ì„œ í•´ë‹¹ weightëŠ” ì•„ë˜ì™€ ê°™ì€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ê²Œ ë©ë‹ˆë‹¤.

$$
 N(0,{2\over n_{in}+n_{out}})
$$

&nbsp; &nbsp;ì½”ë“œë¡œëŠ” ì•„ë˜ì™€ ê°™ì´ ì‘ì„±í•´ì£¼ë©´ ë©ë‹ˆë‹¤.

```python
# xavier initialization
W = np.random.randn(fan_in, fan_out) / np.sqrt((fan_in+fan_out)/2)
```

&nbsp; &nbsp; ìœ„ì˜ ì‹ì„ ë³´ê²Œë˜ë©´ $N(0,1)$ì¸ í‘œì¤€ ì •ê·œ ë¶„í¬ì—ì„œ samplingí•œ $W$ë¥¼ $\sqrt{2/(in+out)}$ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” 1ì˜ í‘œì¤€ í¸ì°¨ë¥¼ ê°€ì§„ ë¶„í¬ë¥¼ $\sqrt {2 \over n_{in}+n_{out}}$ì˜ í‘œì¤€í¸ì°¨ë¥¼ ê°–ëŠ” ë¶„í¬ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ì„œëŠ” $\sqrt {2 \over n_{in}+n_{out}}$ë¥¼ ê³±í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

&nbsp; &nbsp; Xavier initializationì€ **non-linear activation function**ì¸ $tanh(x)$ì™€ $sigmoid(x)$ì—ì„œ **ì¢‹ì€ ì„±ëŠ¥**ì„ ë³´ì´ì§€ë§Œ ReLUì™€ ì‚¬ìš©í•  ë•Œ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ì— ReLUë¥¼ ì‚¬ìš©í•  ë• **He initializationì„ ì‚¬ìš©**í•©ë‹ˆë‹¤.

â‘¡ He initialization

&nbsp; &nbsp; He initializationì€ **xavier initializationê³¼ ìœ ì‚¬**í•©ë‹ˆë‹¤. ì •ê·œ ë¶„í¬ì™€ ê· ë“± ë¶„í¬ ë‘ê°€ì§€ë¡œ ë‚˜ë‰˜ê²Œ ë˜ëŠ”ë° He initializationëŠ” xavierì™€ ë‹¤ë¥´ê²Œ ì´ì „ ì¸µì˜ ë‰´ëŸ° ìˆ˜ë§Œ ë°˜ì˜í•˜ê³  ë‹¤ìŒ ì¸µì˜ ë‰´ëŸ° ìˆ˜ë¥¼ ë°˜ì˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

&nbsp; &nbsp; weightë¥¼ uniform dstributionìœ¼ë¡œ ì´ˆê¸°í™”í•  ê²½ìš° ì•„ë˜ì™€ ê°™ì€ ê· ë“± ë¶„í¬ ë²”ìœ„ë¥¼ ê°€ì§€ë„ë¡ í•©ë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\7.png"/>
</p>

&nbsp; &nbsp; í•˜ì§€ë§Œ, ì •ê·œ ë¶„í¬ë¡œ ì´ˆê¸°í™”í•  ê²½ìš° ì•„ë˜ì™€ ê°™ì€ $\sigma$ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤

<p align="center">
   <img src="assets\2021-08-21\8.png"/>
</p>
&nbsp; &nbsp; ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë©´ He initializationì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# He initialization
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)
# W = np.random.randn(fan_in, fan_out) * np.sqrt(2/fan_in)
# In Pytorch...
torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')
```

## More Details about Xavier initialization & He initialization

&nbsp; &nbsp; **Xavier initialization**ì€ **tanhë‚˜ sigmoidí˜•íƒœì˜ ë¹„ì„ í˜• í•¨ìˆ˜ì—ì„œ ì˜ ë™ì‘**í•©ë‹ˆë‹¤. ì‰½ê²Œ ì„¤ëª…í•˜ë©´ **sigmoidì™€ tanhì˜ saturationì˜ì—­ìœ¼ë¡œ activation valueë“¤ì´ ë“¤ì–´ê°€ì§€ ì•Šë„ë¡ ì„¤ì •**í•´ì£¼ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¦‰, sigmoidì™€ tanhì˜ **linear ì˜ì—­ì— activation valueë“¤ì´ ë“¤ì–´ê°€ë„ë¡** í•˜ë©´ì„œ **saturationì— ë¹ ì§€ì§€ ì•Šë„ë¡ activation valueë¥¼ ì¡°ì •**í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ hidden layerê°€ 3ê°œê°€ ë„˜ì–´ê°€ **4ê°œê°€ ëœë‹¤ë©´ í•™ìŠµì´ ì œëŒ€ë¡œ ì§„í–‰ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ì•„ë˜ì˜ ê·¸ë¦¼ì€ sigmoid activation functionì„ í†µê³¼í•œ ê°ê°ì˜ layerì˜ activation valueì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤.

<p align="center">
   <img src="assets\2021-08-21\9.png"/>
</p>

&nbsp; &nbsp; ì´ëŸ° í˜„ìƒì´ ì¼ì–´ë‚˜ëŠ” ì´ìœ ëŠ” **ê° layerë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ activation functionì„ í†µê³¼í•œ acitivation valueë“¤ì˜ ë¶„í¬ê°€ linearí•œ ì˜ì—­ì— ìˆì§€ ëª»í•˜ê³  saturationì˜ì—­ì— ë“¤ì–´ê°€ ë¶„í¬ê°€ í”ë“¤ë¦¬ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ìœ„ì˜ ê°ê°ì˜ ê·¸ë˜í”„ëŠ” í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 4ê°œì˜ layerë¥¼ ì§€ë‚˜ê²Œ ë˜ë©´ sigmid activationì„ í†µê³¼í•œ activation valueë“¤ì˜ idealí•œ í‰ê· ì¸ **0.5ì— í˜„ì €íˆ ëª»ë¯¸ì¹˜ëŠ” ê²ƒ**ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&nbsp; &nbsp; ì´ì— ê° layerì˜ activation functionì„ í†µê³¼í•œ **activation valueë“¤ì˜ ë¶„í¬ë¥¼ ëª¨ë‘ ë˜‘ê°™ì´(meanê³¼ standard deviationì„ ë™ì¼í•˜ê²Œ) ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•´**ì„œ **"ì´ì „ layerì˜ input ê°¯ìˆ˜($fan_{in}$)ì™€ ë‹¤ìŒ layerì˜ output ê°œìˆ˜($fan_{out}$)ë¥¼ í†µí•´ weightë¥¼ ì´ˆê¸°í™”"** í•©ë‹ˆë‹¤. ì´ì „ layerì˜ inputê°œìˆ˜ë¥¼ í†µí•´ì„œ ì„¤ì •í•˜ëŠ” ì´ìœ ëŠ” activation valueë“¤ì„ sigmoidë‚˜ tanhì˜ saturation ì˜ì—­ìœ¼ë¡œ ë³´ë‚´ì§€ ì•Šê³  **linearí•œ ì˜ì—­ìœ¼ë¡œ ê°€ì ¸ê°€ê¸° ìœ„í•¨**ì´ê³  **output ê°œìˆ˜ë¥¼ í†µí•´ weightë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì´ìœ **ëŠ” **backpropagation**ê³¼ ê´€ë ¨ì´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•´ì„œ ë‚˜ì˜¨ initializationì´ **Xavier initialization**ì…ë‹ˆë‹¤. Xavier initializationëŠ” weightì˜ í‘œì¤€í¸ì°¨ë¥¼ $\sqrt{2\over fan_{in}+fan_{out}}$ì´ ë˜ê²Œ ë§Œë“œëŠ”ë° ì´ëŠ” $**fan_{in}$ê³¼ $fan_{out}$ì˜ ì¡°í™”í‰ê· \*\*ì…ë‹ˆë‹¤.

&nbsp; &nbsp; í•˜ì§€ë§Œ Xavier initializationì€ ì•„ê¹Œ ì„¤ëª…í–ˆë˜ ê²ƒ ì²˜ëŸ¼ **sigmoidë‚˜ tanhì™€ ê°™ì€ ë¹„ì„ í˜• í•¨ìˆ˜ì˜ ì„ í˜•ë¶€ë¶„ì— activation valueë“¤ì„ mappingì‹œì¼œì£¼ì–´ forwardì™€ backpropì´ ì˜ë˜ê²Œ í•´ì¤€ ê²ƒ**ì´ë¼ **ReLUì—ëŠ” ì˜ ì‘ë™ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ì´ì— **He initialization**ì„ ì‚¬ìš©í•´ì¤ë‹ˆë‹¤.

&nbsp; &nbsp; **He intialization**ì€ Xavier initializationì˜ í‘œì¤€í¸ì°¨ì— $\sqrt{2}$ë¥¼ ê³±í•´ì£¼ì–´ **weightì˜ ë¶„í¬ë¥¼ ì¢€ ë” ë„“ì€ í˜•íƒœ**ë¡œ ì¡ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ì™œëƒí•˜ë©´ **ReLUì—ì„œëŠ” ìŒì˜ ì˜ì—­ì— ìˆëŠ” ê°’ë“¤ì€ ëª¨ë‘ activaion functionì„ ì§€ë‚˜ê²Œ ë˜ë©´ 0ì´ ë˜ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ì´ì— $\sqrt{2}$ë°° ë§Œí¼ í‘œì¤€í¸ì°¨ë¥¼ ë” ëŠ˜ë ¤ì£¼ë©´ì„œ **layerë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ ì–‘ì˜ ì˜ì—­ì— ìˆëŠ” activation ê°’ë“¤ì„ ì¢€ ë” í´ì£¼ëŠ” ì—­í• ì„ ìˆ˜í–‰**í•©ë‹ˆë‹¤.

&nbsp; &nbsp; ì¢€ ë” ë‹¤ë¥´ê²Œ í•´ì„í•´ ë³¸ë‹¤ë©´ ì§ê´€ì ìœ¼ë¡œ ë´¤ì„ ë•Œ, **ReLUëŠ” ìŒì˜ ì˜ì—­ì— ìˆëŠ” activation valueë“¤ì„ ì—†ì• ë¯€ë¡œì¨ ì…ë ¥ì„ ë°˜í† ë§‰ ë‚¸ë‹¤**ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— $**fan_{in}$ì— 2ë¥¼ ë‚˜ëˆ ì„œ ë§ˆì¹˜ ì‹¤ì œ ì…ë ¥ì˜ ë°˜ë§Œí¼ë§Œ ì…ë ¥ìœ¼ë¡œ ë°˜ì˜\*\*ëœë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— $fan_{in} \over 2$ë¥¼ í•´ì¤€ ê²ƒì…ë‹ˆë‹¤.

&nbsp; &nbsp; ì•„ë˜ì˜ ì‚¬ì§„â‘ ì€ He initialization ì´ ì•„ë‹Œ Xavier initializationì„ ì‚¬ìš©í–ˆì„ ë–„ì˜ ëª¨ìŠµì´ë©° ì‚¬ì§„â‘¡ëŠ” He initialization ì„ ì‚¬ìš©í–ˆì„ ë•Œì˜ activation valueì˜ distributionì…ë‹ˆë‹¤.

<p align = "center">
<ì‚¬ì§„â‘ >
</p>
<p align="center">
   <img src="assets\2021-08-21\10.png"/>
</p>

<p align = "center">
<ì‚¬ì§„â‘¡>
</p>
<p align="center">
   <img src="assets\2021-08-21\11.png"/>
</p>

&nbsp; &nbsp; í•´ë‹¹ sessionê³¼ ê´€ë ¨ëœ ë‚´ìš©(Xavier initialization ì„¤ëª…)ì€ ì•„ë˜ ë§í¬ë¥¼ í†µí•´ ë“¤ì–´ê°€ì‹œë©´ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ê¸€ë§ì„ í•˜ë©° ê³µë¶€í•˜ë˜ ì¤‘ ê°ëª…ê¹Šê²Œ ì½ì€ ë¸”ë¡œê·¸ì…ë‹ˆë‹¤ğŸ‘

link: [https://nittaku.tistory.com/269](https://nittaku.tistory.com/269)

## Code

&nbsp; &nbsp; Xavier, He initializationì„ ë°°ìš°ê³  ë‚˜ì„œ GANì˜ codeì— ì§ì ‘ initializationì„ ì ìš©í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ë‘ ê°œì˜ modelì„ ì„¤ê³„í•˜ì˜€ëŠ”ë° í•˜ë‚˜ëŠ” initializationì„ ê·¸ëƒ¥ randomí•˜ê²Œ ì¤€ ê²ƒì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” He initializationê³¼ Xavier initializationì„ í†µí•´ ì´ˆê¸°í™” í•´ì£¼ì—ˆìŠµë‹ˆë‹¤. **Leaky ReLUë¥¼ ì‚¬ìš©í•œ ë¶€ë¶„ì€ He initilization**ì„, **tanhë‚˜ sigmoidë¥¼ ì‚¬ìš©í•œ ë¶€ë¶„ì€ Xavier initializationì„** ì‚¬ìš©í•´ì£¼ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

<p align="center">
   < Generator Loss >
</p>
<p align="center">
   <img src="assets\2021-08-21\12.png"/>
</p>

&nbsp; &nbsp; ìœ„ì˜ ê·¸ë˜í”„ëŠ” initialization í•´ì¤€ ê·¸ë˜í”„(<span style="color:orange">ì£¼í™©ìƒ‰</span>)ì™€ initilizationì„ í•´ì£¼ì§€ ì•Šì€ ê·¸ë˜í”„(<span style="color:blue">íŒŒë€ìƒ‰</span>) ì…ë‹ˆë‹¤. ì´ê²ƒì€ Generator lossë¡œ initializationì„ í•´ì¤€ ê·¸ë˜í”„ê°€ ì²˜ìŒì—” **lossê°€ ì—„ì²­ë‚˜ê²Œ ë›°ë‹¤ê°€ ë¹ ë¥´ê²Œ ë‚®ì•„ì§€ë©´ì„œ initializationì„ í•´ì£¼ì§€ ì•Šì€ ê·¸ë˜í”„ë³´ë‹¤ lossê°€ ì‘ì•„ì§„ ê²ƒ**ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ê¸‰ê²©í•˜ê²Œ lossê°€ ì˜¬ë¼ê°€ ìˆëŠ” ë¶€ë¶„**ì€ discriminatorì˜ í•™ìŠµì´ ì••ë„ì ìœ¼ë¡œ ì˜ë˜ì–´ discriminatorê°€ **generatorê°€ ë§Œë“  imageë¥¼ ë³´ê³  fake imageë¼ê³  ì˜ íŒë‹¨**í•˜ì˜€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. discriminatorìª½ì—ë„ initializationì„ ì ìš©í•´ì£¼ì—ˆê¸° ë•Œë¬¸ì— ì´ˆë°˜ epoch ì—ì„œ initializationì„ ì ìš©í•œ generatorì˜ lossê°€ initializationì„ ì ìš©í•˜ì§€ ì•Šì€ generatorì˜ lossë³´ë‹¤ ì—„ì²­ë‚˜ê²Œ í° ê²ƒ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì´ˆê¸°í™”ë¥¼ í•´ì¤€ generatorê°€ ì•ˆí•´ì¤€ generator ë³´ë‹¤ í•™ìŠµì´ ê¸‰ì†ë„ë¡œ ì˜ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<p align="center">
   < Discriminator Loss >
</p>

<p align="center">
   <img src="assets\2021-08-21\13.png"/>
</p>

&nbsp; &nbsp; **initializationì„ ì ìš©í•œ ê·¸ë˜í”„ê°€ ì ìš©í•˜ì§€ ì•Šì€ ê·¸ë˜í”„ë³´ë‹¤ ì´ˆë°˜ epochì—ì„œ lossê°€ ë” ë‚®ì€** ê²ƒì„ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ **initializationì„ í†µí•´ discriminatorì˜ ê°€ì¤‘ì¹˜ê°€ ì˜ ì„¤ì •ë˜ì—ˆê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ epochê°€ ì ì  ê²½ê³¼ë˜ë©´ì„œ initializationì´ ì ìš©ëœ discriminatorì˜ lossê°€ **25 epoch ì´í›„ë¡œ ë” ë†’ì€ ê²ƒì„ í™•ì¸**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ initializationì´ ì ìš©ëœ Generatorì˜ í•™ìŠµì´ ì˜ ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. Initializationì„ ì ìš©í•œ discriminatorì˜ lossê°€ initializationì„ ì ìš©í•˜ì§€ ì•Šì€ discriminatorë³´ë‹¤ ë†’ìœ¼ë¯€ë¡œ **initializationì„ ì ìš©í•œ discriminatorê°€ Generatorë¥¼ ë” ì˜ í•™ìŠµ**ëœ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<p align="center">
   < Generator Accuracy >
</p>
<p align="center">
   <img src="assets\2021-08-21\14.png"/>
</p>
&nbsp; &nbsp; ìœ„ì˜ ê·¸ë¦¼ì€ epochê°€ ì§€ë‚¨ì— ë”°ë¼, ë‘ ê°œì˜ modelì´ í•™ìŠµí•˜ê³ ìˆëŠ” Generatorì˜ accuracy ë³€í™”ë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. Initializationì„ ì‚¬ìš©í•œ modelì´ accuracyê°€ ë” ë†’ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&nbsp; &nbsp; ë”°ë¼ì„œ ì´ ì‹¤í—˜ì„ í†µí•´ **initializationì„ ì‚¬ìš©**í•˜ë©´ **ëª¨ë¸ì´ ì¢€ ë” ë¹¨ë¦¬ í•™ìŠµë  ë¿ë§Œ ì•„ë‹ˆë¼ performanceë„ ë” ì¢‹ì•„ì§„ë‹¤ëŠ” ê²ƒ**ì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ìœ„ì˜ ì‹¤í—˜ ê²°ê³¼ì— ëŒ€í•œ ì½”ë“œëŠ” ì•„ë˜ì˜ ë§í¬ë¥¼ í†µí•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ğŸ˜‰

link:

## Summary

1. Weight Intialization Problemì€ **weightê°€ ë„ˆë¬´ ì‘ê±°ë‚˜ í¬ê²Œë˜ë©´ ë¬¸ì œê°€ ë°œìƒ**í•©ë‹ˆë‹¤.
2. ê° ë ˆì´ì–´ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ í™•ì¸í•˜ë©´ í•™ìŠµì´ ì˜ ë˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ë„ˆë¬´ ê·¹ë‹¨ì ì¸ ë¶„í¬ë¥¼ ê°–ê²Œ ëœë‹¤ë©´(0ìœ¼ë¡œ ì¹˜ì¤‘ë˜ê±°ë‚˜ -1ê³¼ 1ë¡œ í¼ì§€ëŠ” í˜„ìƒ) í•™ìŠµì´ ì˜ ì§„í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. **layerì˜ output ê°’ë“¤ì´ 0ìœ¼ë¡œ ì¹˜ì¤‘**ë˜ëŠ” ê²½ìš°, ${\vartheta loss \over \vartheta w} = {\vartheta loss \over \vartheta f} * {\vartheta f \over \vartheta g} * {\vartheta g \over \vartheta w}$ë¥¼ êµ¬í•  ë•Œ local gradientì¸ ${\vartheta g \over \vartheta w}$ëŠ” 0ì´ ë©ë‹ˆë‹¤(local gradient ${\vartheta g \over \vartheta w} = x=0$).ì´ì— ë˜ë©´ ${\vartheta loss \over \vartheta w}$ ê°€ 0 ì´ë˜ì–´ í•™ìŠµì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
   **Layerì˜ output ê°’ë“¤ì´ -1 ì´ë‚˜ 1ë¡œ ì¹˜ì¤‘ë˜ëŠ” ê²½ìš°**, activation functionì˜ saturation areaì˜ gradient(around 0)ê°€ chain ruleì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ì— ${\vartheta loss \over \vartheta w}$ ê°€ 0ì´ ë˜ì–´ í•™ìŠµì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3. xavier initializationì€ fan_inê³¼ fan_outì„ ì‚¬ìš©í•˜ë©° tanh, sigmoidì— ì ìš©í•©ë‹ˆë‹¤. He initializationì€ fan_inì„ ì‚¬ìš©í•˜ë©° ReLUì— ì ìš©í•©ë‹ˆë‹¤.
4. He initializationì€ Xavier initializationì´ ë§Œë“¤ì–´ì£¼ëŠ” weight ë¶„í¬ì˜ $\sigma$(í‘œì¤€í¸ì°¨)ë³´ë‹¤ $\sqrt{2}$ë°° ë§Œí¼ ë” ì»¤ì„œ **activation valueì˜ ë¶„í¬ë¥¼ ì¢Œìš°ë¡œ** ë” ëŠ˜ë ¤ì£¼ê²Œ ë©ë‹ˆë‹¤.

## Reference

- Xavier intialization: [https://reniew.github.io/13/](https://reniew.github.io/13/)
- Xavier initialization ì„¤ëª… ë° ì‚¬ì§„ ì¸ìš©: [https://nittaku.tistory.com/269](https://nittaku.tistory.com/269)
- He initialization ì„¤ëª… ë° ì‚¬ì§„ ì¸ìš©: [https://excelsior-cjh.tistory.com/m/177](https://excelsior-cjh.tistory.com/m/177)
