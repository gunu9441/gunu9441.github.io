---
title: Frobenius Norm
date: 2021-10-17 19:10:74
category: AI
thumbnail: { thumbnailSrc }
draft: false
---

ì´ë²ˆ ì‹œê°„ì—ëŠ” Regularization(ì •ê·œí™”)ì˜ í•˜ë‚˜ì˜ ê¸°ë²•ì¸ **Frobenius Norm**ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤!ğŸ™Œ ê¸°ë³¸ì ìœ¼ë¡œ Regularization(ì •ê·œí™”)ì€ **overfittingì„ ë§‰ê¸° ìœ„í•´ ì§„í–‰**ë©ë‹ˆë‹¤. **Overfitting**ì´ë€ ëª¨ë¸ì´ ë¶ˆí•„ìš”í•˜ê²Œ **training datasetì˜ noiseê¹Œì§€ í•™ìŠµ**í•˜ì—¬ **ìƒˆë¡œìš´ ë°ì´í„° ì…‹**(test or validation set)**ì— ëŒ€í•´ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒ**ì„ ë§í•©ë‹ˆë‹¤. Overfittingì— ëŒ€í•´ ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•˜ì‹  ë¶„ì€ (ì—¬ê¸°)[]ë¥¼ ì°¸ê³ í•˜ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤! ì•„ë˜ëŠ” ì´ë²ˆ í¬ìŠ¤íŒ…ì— ëŒ€í•œ Categoryì…ë‹ˆë‹¤.

## Category

1. Basic Cost Function
2. L1 Regularization & L2 Regularization
3. Frobenius Norm
4. Weight Decay
5. The Principle of Regularization
6. Conclusion
7. Summary
8. Reference

## Basic Cost Function

Deep learningì—ì„œëŠ” cost functionì„ ì •ì˜í•˜ì—¬ labelê³¼ predictionì‚¬ì´ì˜ ì°¨ì´ê°€ ì–¼ë§ˆë‚˜ ë‚˜ëŠ”ì§€ ì¸¡ì •ì„ í•˜ê²Œ ë˜ëŠ”ë° ì´ë¥¼ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

$$
Loss={1\over m}\sum_{i=1}^m L(\hat y^{(i)}, y^{(i)})
$$

ìœ„ì˜ ìˆ˜ì‹ì„ í†µí•´ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ê°ê°ì˜ input dataì— ëŒ€í•œ modelì˜ prediction ê°’ê³¼ labelê°„ì˜ Lossë¥¼ ì¸¡ì •í•˜ì—¬ ë‹¤ ë”í•´ì„œ í‰ê· ì„ ì·¨í•œ ê²ƒì´ $Loss$ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ Loss functionì— L1 normê³¼ L2 normì„ ì¶”ê°€ì‹œì¼œ regularizationì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## L1 Regularization & L2 Regularization

L1 Regularizationê³¼ L2 Regularizationì„ ì•Œì•„ë³´ê¸° ì „ì— **"norm"**ì´ë€ ë¬´ì—‡ì¼ê¹Œìš”? ì„ í˜•ëŒ€ìˆ˜ì—ì„œì˜ normì€ **vector spaceìƒì˜ vectorì˜ í¬ê¸°** ë˜ëŠ” **ê¸¸ì´ë¥¼ ì¸¡ì •í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ê°œë…**ì…ë‹ˆë‹¤. normì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
||x||_p=(\sum_{i=1}^n |x_i^p|)^{1\over p}
$$

ìœ„ì˜ ìˆ˜ì‹ì„ ë³´ê²Œ ë˜ë©´ **ì ˆëŒ“ê°’ì„ ì”Œì›Œì£¼ê³  ë”í•˜ë¯€ë¡œ ë¬´ì¡°ê±´ ì–‘ìˆ˜** ê°’ì´ ë‚˜ì˜¤ê²Œë©ë‹ˆë‹¤. ì´ì— ê¸¸ì´ë‚˜ í¬ê¸°ë¥¼ ì¸¡ì •í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. normì„ regularizationì— ì‚¬ìš©í•˜ëŠ”ë° **Logistic Regressionì—ì„œ L2 norm(ìœ í´ë¦¬ë“œ ë…¸ë¦„)**ì„ ì‚¬ìš©í•œ **L2 regularizationì˜ ìˆ˜ì‹**ë¶€í„° ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

$$
{\lambda \over 2m}||w||_2^2
$$

ìœ„ì˜ ìˆ˜ì‹ì€ **ê¸°ì¡´ì˜ Loss functionì— L2 normì˜ ì œê³±ì„ ë”í•´ì¤€** í˜•íƒœì…ë‹ˆë‹¤. $||w||_2^2$ì˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
||w||_2^2=((\sum_{i=1}^n |w_i^2|)^{1\over 2})^2 = \sum_{i=1}^n |w_i^2|=w^Tw
$$

ìœ„ì˜ ìˆ˜ì‹ì„ ë³´ê²Œë˜ë©´ $||w||_2^2$ì€ **L2 normì˜ ì œê³±**ì„ ì˜ë¯¸í•˜ë©° ì´ê²ƒì€ ê²°êµ­ **ëª¨ë“  ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©**ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ê²ƒì„ **L2 Regularization**ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. L2 Regularizationì€ **ê°€ì¥ í”í•œ regularization ê¸°ë²•**ì…ë‹ˆë‹¤.

L1 Regularizationì€ **L1 normì„ ì‚¬ìš©í•´ì„œ êµ¬í˜„**í•˜ê²Œ ë˜ë©° ì•„ë˜ì˜ ìˆ˜ì‹ì—ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
{\lambda \over 2m} ||w||_1={\lambda \over 2m}{\sum_{i=1}^n}|w_i|
$$

í•˜ì§€ë§Œ **L1 Regularizationì„ ì‚¬ìš©**í•˜ê²Œ ë˜ë©´ **0ì´ ë˜ëŠ” weightê°€ ë§ì•„ì§€ê²Œ ë©ë‹ˆë‹¤**. ì–´ë–¤ ì‚¬ëŒë“¤ì€ ì´ íš¨ê³¼ê°€ **ëª¨ë¸ì„ ì••ì¶•ì‹œí‚¤ëŠ”ë° ë„ì›€ì´ ëœë‹¤**ê³  ë§ì„ í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ **ëª‡ëª‡ì˜ parameter(weight)ê°€ 0**ê°€ ë˜ê¸° ë•Œë¬¸ì— **ì €ì¥í•˜ëŠ”ë° ë©”ëª¨ë¦¬ê°€ ì ê²Œ í•„ìš”í•˜ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ì„ sparse(í¬ì†Œ)í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” **L1 regularizationì´ ë„ì›€ì´ ë˜ì§€ ì•Šê¸° ë•Œë¬¸**ì— **ëª¨ë¸ì„ ì••ì¶•í•˜ê² ë‹¤ëŠ” ëª©í‘œê°€ ìˆì§€ ì•ŠëŠ” ì´ìƒ L1 regularizationì„ ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤**ê³ í•©ë‹ˆë‹¤.

ì´ì— ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ ì‹œí‚¬ ë•ŒëŠ” **L2 Regularizationì„ í›¨ì”¬ ë” ë§ì´ ì‚¬ìš©**í•©ë‹ˆë‹¤. ì´ L2 Regulairzationì„ ì‚¬ìš©í•  ë•Œ $\lambda$ë¼ëŠ” hyperparameterë¥¼ ì„¤ì •í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.

## Frobenius Regularization

Neural networkì—ì„œ **frobenius regularizationì„ ì‚¬ìš©**í•˜ê²Œ ë©ë‹ˆë‹¤.

$$
Loss(w^{[i]},b^{[i]},...,w^{[L]},b^{[L]})={1 \over m}\sum_{i=1}^{m}L(\hat y^{(i)}, y^{(i)})+{\lambda \over 2m}\sum_{l=1}^L||w^{[l]}||_F^2
$$

ìœ„ì˜ ìˆ˜ì‹ì€ Frobenius normì„ í†µí•˜ì—¬ regularization termì„ ì¶”ê°€ì‹œí‚¨ cost functionì´ë©° Frobenius normì˜ ì œê³±ì„ í’€ì–´ì„œ ì ê²Œë˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
||w^{[l]}||^2_F=\sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2,~~~~~w:(n^{[l-1]},n^{[l]})
$$

ìœ„ì— ë‚˜ì˜¨ í–‰ë ¬ì˜ norm(ë…¸ë¦„)ì„ ìš°ë¦¬ëŠ” **frobenius norm**ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. ì„ í˜• ëŒ€ìˆ˜í•™ì—ì„œëŠ” ê´€ë¡€ì— ë”°ë¼ **í–‰ë ¬ì˜ L2 normì´ë¼ê³  ë¶€ë¥´ì§€ ì•Šê³  frobenius norm**ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. **Frobenius norm**ê³¼ ìœ„ì—ì„œ ë°°ì› ë˜ ë²¡í„°ì˜ **L2 norm**ì€ **elementë“¤ì„ ì „ë¶€ ì œê³±ì‹œì¼œ ë”í•˜ê¸° ë•Œë¬¸ì—** ë™ì‘ì´ ê°™ë‹¤ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ì§€ ì œê³±ì‹œí‚¤ê³  ë”í•˜ëŠ” **ëŒ€ìƒì´ frobenius normì€ í–‰ë ¬**, **L2 normì€ ë²¡í„°**ë¼ëŠ” ì°¨ì´ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤. ì°¸ê³ ë¡œ $w$ì˜ shapeëŠ” (ì´ì „ ë ˆì´ì–´ì˜ ë‰´ëŸ° ê°¯ìˆ˜, í˜„ì¬ ë ˆì´ì–´ì˜ ë‰´ëŸ° ê°¯ìˆ˜)ì´ë¯€ë¡œ $(n^{[l-1]},n^{[l]})$ì…ë‹ˆë‹¤.

ì´ì œ frobenius normì„ ì‚¬ìš©í•˜ì—¬ regularizationì„ ì ìš©í•œ cost functionì„ ë‹¤ì‹œ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

$$
Loss(w^{[i]},b^{[i]},...,w^{[L]},b^{[L]})={1 \over m}\sum_{i=1}^{m}L(\hat y^{(i)}, y^{(i)})+{\lambda \over 2m}\sum_{l=1}^L||w^{[l]}||^2
$$

ìœ„ì˜ cost functionì„ $w^{[l]}$-ì— ëŒ€í•´ í¸ë¯¸ë¶„í•˜ì—¬ gradient descentë¥¼ ì‚¬ìš©í•˜ê²Œë˜ë©´ ì•„ë˜ì™€ ê°™ì´ updateë˜ê²Œ ë©ë‹ˆë‹¤.

$$
w^{[l]}=w^{[l]}-\alpha * {\vartheta Loss \over \vartheta w^{[l]}} = w^{[l]}-\alpha *(gradient_{origin}+{\lambda \over m}w^{[l]})
$$

ê¸°ì¡´ì˜ vanilla gradient descentëŠ” $w^{[l]}=w^{[l]}-\alpha *(gradient_{origin})$ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ìœ„ì˜ ìˆ˜ì‹ì„ ë³´ê²Œë˜ë©´ cost functionì— frobenius normì˜ ì œê³±ì´ ì¶”ê°€í•¨ì— ë”°ë¼ ê¸°ì¡´ì˜ gradient($gradient_{origin}$)ì— ${\lambda \over m}w^{[l]}$ì´ ë”í•´ì ¸ì„œ $w^{[l]}$ì´ updateë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Weight Decay

ìš°ë¦¬ê°€ í”íˆ ë¶€ë¥´ëŠ” L2 regularizationì€ ì‚¬ì‹¤ frobenius regularizationì…ë‹ˆë‹¤. Frobenius regularizationì€ ë‹¤ë¥¸ ë§ë¡œ **weight decay**ë¼ê³  ë¶€ë¥´ê²Œ ë©ë‹ˆë‹¤.

$$
w^{[l]}=w^{[l]}-\alpha * {\vartheta Loss \over \vartheta w^{[l]}} = w^{[l]}-\alpha *(gradient_{origin}+{\lambda \over m}w^{[l]})
$$

$$
= w^{[l]}-\alpha *gradient_{origin} - \alpha * {\lambda \over m}w^{[l]} = (1-{\alpha\lambda \over m})w^{[l]}-\alpha *gradient_{origin}
$$

ìœ„ì˜ ìˆ˜ì‹ì„ ë³´ê²Œ ë˜ë©´ $(1-{\alpha\lambda \over m})w^{[l]}$ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. $\alpha *gradient_{origin}$ì„ ë¹¼ì£¼ê¸° ì „ì— ì›ë˜ì˜ $w^{[l]}$ì„ ê°ì†Œì‹œì¼°ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— ì²˜ìŒë¶€í„°$(1-{\alpha\lambda \over m})$(â†’1ë³´ë‹¤ ì‘ì€ ê°’ì„ ê³±í•´ì£¼ê²Œ ë˜ë©´ ì›ë˜ ê°’ë³´ë‹¤ ë‹¹ì—°íˆ ì‘ì•„ì§€ê² ì£ ?!ğŸ˜Š)ì„ í†µí•´ $w^{[l]}$ì„ ê°ì†Œì‹œí‚¤ë¯€ë¡œ **L2 norm regularizationê³¼ frobenius norm regularization**ì„ **weight decay**ë¼ê³  ë¶€ë¥¸ë‹¤ê³  í•©ë‹ˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ L2 norm regularizationê³¼ frobenius norm regularizationì´ overfittingì„ ë§‰ì„ ìˆ˜ ìˆì„ê¹Œìš”??

## The Principle of Regularization

frobenius norm regularizationì„ í†µí•œ $w^{[l]}$ì„ updateí•˜ëŠ” ìˆ˜ì‹ì„ ë‹¤ì‹œ ì‚´í´ë´…ì‹œë‹¤.

$$
(1-{\alpha\lambda \over m})w^{[l]}-\alpha *gradient_{origin}
$$

ìœ„ì˜ ì‹ì„ ë³´ê²Œ ë˜ë©´ $\lambda$ê°€ ì»¤ì§€ë©´ ì»¤ì§ˆ ìˆ˜ë¡ $w^{[l]}$ì„ **0ì— ê°€ê¹ê²Œ ì„¤ì •**í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ì€ $w$ë“¤ì´ 0ì— ê°€ê¹ê²Œ ì„¤ì •ë˜ë©´ ì–´ë–¤ ì¼ì´ ë°œìƒí• ê¹Œìš”?

ë‹¹ì—°íˆ $w^{[l]}$ì´ **0ì— ê°€ê¹Œì›Œì§€ë©´** hidden layerì•ˆì— ì¡´ì¬í•˜ëŠ” node($w$ê°€ ì‹œì‘ë˜ëŠ” node)ì˜ **ì˜í–¥ë ¥ì´ ì¤„ì–´ë“¤ê²Œ ë  ê²ƒ**ì…ë‹ˆë‹¤. hidden layerì•ˆì— **ë‹¤ì–‘í•œ nodeë“¤ì˜ ì˜í–¥ë ¥ì´ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸**ì— ë³´ì´ëŠ” ê²ƒë³´ë‹¤ëŠ” **ë” ì‘ê³  ê°„ë‹¨í•œ ì‹ ê²½ë§**ì´ ë©ë‹ˆë‹¤. ì´ì— overfittingë  ìˆ˜ ìˆëŠ” deep modelì„ **ë” ê°„ë‹¨í•˜ê²Œ** ë§Œë“¤ì–´ì£¼ì–´ **overfittingì´ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµ**ë‹ˆë‹¤.

í•˜ì§€ë§Œ ì—¬ê¸°ì„œ ì¤‘ìš”í•œì ì€ ë ˆì´ì–´ ì•ˆì— ìˆëŠ” **nodeë“¤ì„ ì™„ì „íˆ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹ˆë¼** **0ì— ê°€ê¹Œìš´ ìˆ˜ë¡œ ë§Œë“¬**ìœ¼ë¡œì¨ **ëª¨ë“  nodeë¥¼ ì‚¬ìš©**í•˜ì§€ë§Œ **ê°ê°ì˜ ì˜í–¥ë ¥ì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©ì **ìœ¼ë¡œ í•œë‹¤ê³  ìƒê°í•´ì•¼ í•©ë‹ˆë‹¤.

í•˜ì§€ë§Œ ë„ˆë¬´ í° $\lambda$ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ í•™ìŠµí•  ìˆ˜ë¡ **ë§ì€ nodeê°€ 0ì— ê°€ê¹Œì›Œì ¸** ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ **underfitting(high bias)**ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì ì ˆí•œ $\lambda$ê°’ì„ ê°–ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

## Summary

1. $||w||_2^2=L2~norm^2=\sum_{i=1}^n|w_i|^2$
2. Frobenius normì€ ê´€ë¡€ìƒì˜ ì´ìœ ë¡œ í–‰ë ¬ì˜ L2 normì´ë¼ê³  ë¶ˆë¦¬ì§„ ì•ŠìŠµë‹ˆë‹¤. Frobenius norm ìì²´ê°€ í–‰ë ¬ì˜ ì›ì†Œ ì œê³± í•©ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. L2 normì€ ë‹¹ì—°íˆ ë²¡í„°ì˜ ì›ì†Œ ì œê³± í•©ì„ ì˜ë¯¸í•˜ê² ì£ ?!ğŸ˜Š
3. Frobenius norm regulairzation ì—ì„œ $\lambda$ê°’ì„ í¬ê²Œ ê°€ì ¸ê°€ë©´ $w$ê°€ 0ì— ê°€ê¹ê²Œ ì¤„ì´ë¯€ë¡œì¨ ì¢€ ë” ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ê²Œ ë©ë‹ˆë‹¤.

## Reference

- what is norm?: [https://www.youtube.com/watch?v=yoD5tQ1HQRU](https://www.youtube.com/watch?v=yoD5tQ1HQRU)
- Regularization(C2W1L04)
- Is it fine to use L2 regularization and dropout?\_1: [https://stats.stackexchange.com/questions/241001/deep-learning-use-l2-and-dropout-regularization-simultaneously](https://stats.stackexchange.com/questions/241001/deep-learning-use-l2-and-dropout-regularization-simultaneously)
- Is it fine to use L2 regularization and dropout?\_2: [https://www.quora.com/Is-using-dropout-and-L2-regularization-on-each-layer-of-a-neural-network-overkill-or-good-practice](https://www.quora.com/Is-using-dropout-and-L2-regularization-on-each-layer-of-a-neural-network-overkill-or-good-practice)
